<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group>
<journal-title xml:lang="en">PLoS ONE</journal-title></journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PONE-D-13-03858</article-id>
<article-id pub-id-type="doi">10.1371/journal.pone.0073791</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories>
<title-group>
<article-title>Personality, Gender, and Age in the Language of Social Media: The Open-Vocabulary Approach</article-title>
<alt-title alt-title-type="running-head">Personality, Gender, Age in Social Media Language</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Schwartz</surname><given-names>H. Andrew</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Eichstaedt</surname><given-names>Johannes C.</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Kern</surname><given-names>Margaret L.</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Dziurzynski</surname><given-names>Lukasz</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Ramones</surname><given-names>Stephanie M.</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Agrawal</surname><given-names>Megha</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Shah</surname><given-names>Achal</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Kosinski</surname><given-names>Michal</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Stillwell</surname><given-names>David</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Seligman</surname><given-names>Martin E. P.</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Ungar</surname><given-names>Lyle H.</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><label>1</label><addr-line>Positive Psychology Center, University of Pennsylvania, Philadelphia, Pennsylvania, United States of America</addr-line></aff>
<aff id="aff2"><label>2</label><addr-line>Computer &amp; Information Science, University of Pennsylvania, Philadelphia, Pennsylvania, United States of America</addr-line></aff>
<aff id="aff3"><label>3</label><addr-line>The Psychometrics Centre, University of Cambridge, Cambridge, United Kingdom</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Preis</surname><given-names>Tobias</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1" /></contrib>
</contrib-group>
<aff id="edit1"><addr-line>University of Warwick, United Kingdom</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">hansens@sas.upenn.edu</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: HAS JCE MLK LHU. Performed the experiments: HAS LD. Analyzed the data: HAS JCE LD SMR MA AS. Contributed reagents/materials/analysis tools: MK DS. Wrote the paper: HAS JCE MLK DS MEPS LHU.</p></fn>
</author-notes>
<pub-date pub-type="collection"><year>2013</year></pub-date>
<pub-date pub-type="epub"><day>25</day><month>9</month><year>2013</year></pub-date>
<volume>8</volume>
<issue>9</issue>
<elocation-id>e73791</elocation-id>
<history>
<date date-type="received"><day>23</day><month>1</month><year>2013</year></date>
<date date-type="accepted"><day>29</day><month>7</month><year>2013</year></date>
</history>
<permissions>
<copyright-year>2013</copyright-year>
<copyright-holder>Schwartz et al</copyright-holder><license xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>We analyzed 700 million words, phrases, and topic instances collected from the Facebook messages of 75,000 volunteers, who also took standard personality tests, and found striking variations in language with personality, gender, and age. In our <italic>open-vocabulary</italic> technique, the data itself drives a comprehensive exploration of language that distinguishes people, finding connections that are not captured with traditional closed-vocabulary word-category analyses. Our analyses shed new light on psychosocial processes yielding results that are face valid (e.g., subjects living in high elevations talk about the mountains), tie in with other research (e.g., neurotic people disproportionately use the phrase ‘sick of’ and the word ‘depressed’), suggest new hypotheses (e.g., an active life implies emotional stability), and give detailed insights (males use the possessive ‘my’ when mentioning their ‘wife’ or ‘girlfriend’ more often than females use ‘my’ with ‘husband’ or 'boyfriend’). To date, this represents the largest study, by an order of magnitude, of language and personality.</p>
</abstract>
<funding-group><funding-statement>Support for this research was provided by the Robert Wood Johnson Foundation's Pioneer Portfolio, through a grant to Martin Seligman, “Exploring Concept of Positive Health”. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="16" /></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>The social sciences have entered the age of data science, leveraging the unprecedented sources of written language that social media afford <xref ref-type="bibr" rid="pone.0073791-Lazer1">[1]</xref>–<xref ref-type="bibr" rid="pone.0073791-Miller1">[3]</xref>. Through media such as Facebook and Twitter, used regularly by more than 1/7<sup>th</sup> of the world's population <xref ref-type="bibr" rid="pone.0073791-Facebook1">[4]</xref>, variation in mood has been tracked diurnally and across seasons <xref ref-type="bibr" rid="pone.0073791-Golder1">[5]</xref>, used to predict the stock market <xref ref-type="bibr" rid="pone.0073791-Bollen1">[6]</xref>, and leveraged to estimate happiness across time <xref ref-type="bibr" rid="pone.0073791-Kramer1">[7]</xref>, <xref ref-type="bibr" rid="pone.0073791-Dodds1">[8]</xref>. Search patterns on Google detect influenza epidemics weeks before CDC data confirm them <xref ref-type="bibr" rid="pone.0073791-Ginsberg1">[9]</xref>, and the digitization of books makes possible the quantitative tracking of cultural trends over decades <xref ref-type="bibr" rid="pone.0073791-Michel1">[10]</xref>. To make sense of the massive data available, multidisciplinary collaborations between fields such as computational linguistics and the social sciences are needed. Here, we demonstrate an instrument which uniquely describes similarities and differences among groups of people in terms of their differential language use.</p>
<p>Our technique leverages what people say in social media to find distinctive <italic>words</italic>, <italic>phrases</italic>, and <italic>topics</italic> as functions of known attributes of people such as gender, age, location, or psychological characteristics. The standard approach to correlating language use with individual attributes is to examine usage of <italic>a priori</italic> fixed sets of words <xref ref-type="bibr" rid="pone.0073791-Pennebaker1">[11]</xref>, limiting findings to preconceived relationships with words or categories. In contrast, we extract a data-driven collection of <italic>words</italic>, <italic>phrases</italic>, and <italic>topics</italic>, in which the lexicon is based on the words of the text being analyzed. This yields a comprehensive description of the differences between groups of people for any given attribute, and allows one to find unexpected results. We call approaches like ours, which do not rely on <italic>a priori</italic> word or category judgments, <italic>open-vocabulary</italic> analyses.</p>
<p>We use <italic>differential language analysis</italic> (<italic>DLA</italic>), our particular method of open-vocabulary analysis, to find language features across millions of Facebook messages that distinguish demographic and psychological attributes. From a dataset of over 15.4 million Facebook messages collected from 75 thousand volunteers <xref ref-type="bibr" rid="pone.0073791-Kosinski1">[12]</xref>, we extract 700 million instances of <italic>words</italic>, <italic>phrases</italic>, and automatically generated <italic>topics</italic> and correlate them with gender, age, and personality. We replicate traditional language analyses by applying Linguistic Inquiry and Word Count (<italic>LIWC)</italic> <xref ref-type="bibr" rid="pone.0073791-Pennebaker1">[11]</xref>, a popular tool in psychology, to our data set. Then, we show that <italic>open-vocabulary</italic> analyses can yield additional <italic>insights</italic> (correlations between personality and behavior as manifest through language) and more <italic>information</italic> (as measured through predictive accuracy) than traditional <italic>a priori</italic> word-category approaches. We present a word cloud-based technique to visualize results of <italic>DLA</italic>. Our large set of correlations is made available for others to use (available at: <ext-link ext-link-type="uri" xlink:href="http:www.wwbp.org/" xlink:type="simple">http:www.wwbp.org/</ext-link>).</p>
</sec><sec id="s2">
<title>Background</title>
<p>This section outlines recent work linking language with personality, gender, and age. In line with the focus of this paper, we predominantly discuss works which sought to gain psychological <italic>insights</italic>. However, we also touch on increasingly popular attempts at <italic>predicting</italic> personality from language in social media, which, for our study, offer an empirical means to compare a <italic>closed vocabulary</italic> analysis (relying on <italic>a priori</italic> word category human judgments) and an <italic>open vocabulary</italic> analysis (not relying on <italic>a priori</italic> word category judgments).</p>
<p>Personality refers to the traits and characteristics that make an individual unique. Although there are multiple ways to classify traits <xref ref-type="bibr" rid="pone.0073791-Goldberg1">[13]</xref>, we draw on the popular Five Factor Model (or “Big 5”), which classifies personality traits into five dimensions: <italic>extraversion</italic> (e.g., outgoing, talkative, active), <italic>agreeableness</italic> (e.g., trusting, kind, generous), <italic>conscientiousness</italic> (e.g., self-controlled, responsible, thorough), <italic>neuroticism</italic> (e.g., anxious, depressive, touchy), and <italic>openness</italic> (e.g., intellectual, artistic, insightful) <xref ref-type="bibr" rid="pone.0073791-McCrae1">[14]</xref>. With work beginning over 50 years ago <xref ref-type="bibr" rid="pone.0073791-Norman1">[15]</xref> and journals dedicated to it, the <italic>FFM</italic> is a well-accepted construct of personality <xref ref-type="bibr" rid="pone.0073791-Digman1">[16]</xref>.</p>
<sec id="s2a">
<title>Automatic Lexical Analysis of Personality, Gender, and Age</title>
<p>By examining what words people use, researchers have long sought a better understanding of human psychology <xref ref-type="bibr" rid="pone.0073791-Stone1">[17]</xref>–<xref ref-type="bibr" rid="pone.0073791-Pennebaker2">[19]</xref>. As Tauszczik &amp; Pennebaker put it:</p>
<disp-quote>
<p>Language is the most common and reliable way for people to translate their internal thoughts and emotions into a form that others can understand. Words and language, then, are the very stuff of psychology and communication <xref ref-type="bibr" rid="pone.0073791-Tausczik1">[20]</xref>.</p>
</disp-quote>
<p>The typical approach to analyzing language involves counting word usage over pre-chosen categories of language. For example, one might place words like ‘nose’, ‘bones’, ‘hips’, ‘skin’, ‘hands’, and ‘gut’ into a <italic>body</italic> lexicon, and count how often words in the lexicon are used by <italic>extraverts</italic> or <italic>introverts</italic> in order to determine who talks about the body more. Of such word-category lexica, the most widely used is Linguistic Inquiry and Word Count or <italic>LIWC</italic>, developed over the last couple decades by human judges designating categories for common words <xref ref-type="bibr" rid="pone.0073791-Pennebaker1">[11]</xref>, <xref ref-type="bibr" rid="pone.0073791-Pennebaker2">[19]</xref>. The 2007 version of <italic>LIWC</italic> includes 64 different categories of language ranging from part-of-speech (i.e. <italic>articles</italic>, <italic>prepositions</italic>, <italic>past-tense verbs</italic>, <italic>numbers</italic>,...) to topical categories (i.e. <italic>family</italic>, <italic>cognitive mechanisms</italic>, <italic>affect</italic>, <italic>occupation</italic>, <italic>body</italic>,...), as well as a few other attributes such as total number of words used <xref ref-type="bibr" rid="pone.0073791-Pennebaker1">[11]</xref>. Names of all 64 categories can be seen in <xref ref-type="fig" rid="pone-0073791-g002">Figure 2</xref>.</p>
<p>Pennebaker &amp; King conducted one of the first extensive applications of <italic>LIWC</italic> to personality by examining words in a variety of domains including diaries, college writing assignments, and social psychology manuscript abstracts <xref ref-type="bibr" rid="pone.0073791-Pennebaker3">[21]</xref>. Their results were quite consistent across such domains, finding patterns such as <italic>agreeable</italic> people using more articles, <italic>introverts</italic> and those low in <italic>conscientiousness</italic> using more words signaling distinctions, and <italic>neurotic</italic> individuals using more negative emotion words. Mehl et al. tracks the natural speech of 96 people over two days <xref ref-type="bibr" rid="pone.0073791-Mehl1">[22]</xref>. They found similar results to Pennebaker &amp; King and that <italic>neurotic</italic> and <italic>agreeable</italic> people tend to use more first-person singulars, people low in <italic>openness</italic> talk more about social processes, <italic>extraverts</italic> use longer words.</p>
<p>The recent growth of online social media has yielded great sources of personal discourse. Besides advantages due to the size of the data, the content is often personal and describes everyday concerns. Furthermore, previous research has suggested populations for online studies and Facebook are quite representative <xref ref-type="bibr" rid="pone.0073791-Gosling1">[23]</xref>, <xref ref-type="bibr" rid="pone.0073791-Back1">[24]</xref>. Sumner et al. examined the language of 537 Facebook users with <italic>LIWC</italic> <xref ref-type="bibr" rid="pone.0073791-Sumner1">[25]</xref> while Holtgraves studied the text messages of 46 students <xref ref-type="bibr" rid="pone.0073791-Holtgraves1">[26]</xref>. Findings from these studies largely confirmed past links with LIWC but also introduced some new links such as <italic>neurotics</italic> using more acronyms <xref ref-type="bibr" rid="pone.0073791-Holtgraves1">[26]</xref> or those high in <italic>openness</italic> using more quotations <xref ref-type="bibr" rid="pone.0073791-Sumner1">[25]</xref>.</p>
<p>The larger sample-sizes from social media also enabled the first study exploring personality as a function of single-word use. Yarkoni investigated LIWC categories along with single words in connection with Big-5 scores of 406 bloggers <xref ref-type="bibr" rid="pone.0073791-Yarkoni1">[27]</xref>. He identified single word results which would not have been caught with <italic>LIWC</italic>, such as ‘hug’ correlating positively with <italic>agreeableness</italic> (there is no physical affection category in<italic>LIWC</italic>), but, considering the sparse nature of words, 406 blogs does not result in comprehensive view. For example, they find only 13 significant word correlations for <italic>conscientiousness</italic> while we find thousands even after Bonferonni-correcting significance levels. Additionally, they did not control for age or gender although they reported roughly 75% of their subjects were female. Still, as the most thorough point of comparison for <italic>LIWC</italic> results with personality, <xref ref-type="fig" rid="pone-0073791-g002">Figure 2</xref> presents the findings from Yarkoni's study along with <italic>LIWC</italic> results over our data.</p>
<p>Analogous to a personality construct, work has been done in psychology looking at the latent dimensions of self-expression. Chung and Pennebaker factor analyzed 119 adjectives used in student essays of “who you think you are” and discovered 7 latent dimensions labeled such as “sociability” or “negativity” <xref ref-type="bibr" rid="pone.0073791-Chung1">[28]</xref>. They were able to relate these factors to the Big-5 and found only weak relations, suggesting 7 dimensions as an alternative construction. Later, Kramer and Chung ran the same method over 1000 unique words across Facebook status updates, finding three components labeled, “positive events”, “informal speech”, and “school” <xref ref-type="bibr" rid="pone.0073791-Kramer2">[29]</xref>. Although their vocabulary size was somewhat limited, we still see these as previous examples of open-vocabulary language analyses for psychology – no assumptions were made on the categories of words beyond part-of-speech.</p>
<p><italic>LIWC</italic> has also been used extensively for studying gender and age <xref ref-type="bibr" rid="pone.0073791-Pennebaker3">[21]</xref>. Many studies have focused on function words (articles, prepositions, conjunctions, and pronouns), finding females use more first-person singular pronouns, males use more articles, and that older individuals use more plural pronouns and future tense verbs <xref ref-type="bibr" rid="pone.0073791-Pennebaker4">[30]</xref>–<xref ref-type="bibr" rid="pone.0073791-Argamon1">[32]</xref>. Other works have found males use more formal, affirmation, and informational words, while females use more social interaction, and deictic language <xref ref-type="bibr" rid="pone.0073791-Argamon2">[33]</xref>–<xref ref-type="bibr" rid="pone.0073791-Rao1">[36]</xref>. For age, the most salient findings include older individuals using more positive emotion and less negative emotion words <xref ref-type="bibr" rid="pone.0073791-Pennebaker4">[30]</xref>, older individuals preferring fewer self-references (i.e. ‘I’, ‘me’) <xref ref-type="bibr" rid="pone.0073791-Pennebaker4">[30]</xref>, <xref ref-type="bibr" rid="pone.0073791-Chung2">[31]</xref>, and stylistically there is less use of negation <xref ref-type="bibr" rid="pone.0073791-Schler1">[37]</xref>. Similar to our finding of 2000 topics (clusters of semantically-related words), Argamon et al. used factor analysis and identified 20 coherent components of word use to link gender and age, showing male components of language increase with age while female factors decrease <xref ref-type="bibr" rid="pone.0073791-Argamon1">[32]</xref>.</p>
<p>Occasionally, studies find contradictory results. For example, multiple studies report that emoticons (i.e. ‘:)’ ‘:-(‘) are used more often by females <xref ref-type="bibr" rid="pone.0073791-Newman1">[34]</xref>, <xref ref-type="bibr" rid="pone.0073791-Rao1">[36]</xref>, <xref ref-type="bibr" rid="pone.0073791-Burger1">[38]</xref>, but Huffaker &amp; Calvert found males use them more in a sample of 100 teenage bloggers <xref ref-type="bibr" rid="pone.0073791-Huffaker1">[39]</xref>. This particular discrepancy could be sample-related – differing demographics or having a non-representative sample (Huffaker &amp; Calvert looked at 100 bloggers, while later studies have looked at thousands of twitter users) or it could be due to differences in the domain of the text (blogs versus twitter). One should always be careful generalizing new results outside of the domain they were found as language is often dependent on context <xref ref-type="bibr" rid="pone.0073791-Eckert1">[40]</xref>. In our case we explore language in the broad context of Facebook, and do not claim our results would up under other smaller or larger contexts. As a starting point for reviewing more psychologically meaningful language findings, we refer the reader to Tauszczik &amp; Pennebaker's 2010 survey of computerized text analysis <xref ref-type="bibr" rid="pone.0073791-Tausczik1">[20]</xref>.</p>
<p>Eisenstein et al. presented a sophisticated <italic>open-vocabulary</italic> language analysis of demographics <xref ref-type="bibr" rid="pone.0073791-Eisenstein1">[41]</xref>. Their method views language analysis as a multi-predictor to multi-output regression problem, and uses an L1 norm to select the most useful predictors (i.e. words). Part of their motivation was finding interpretable relationships between individual language features and sets of outcomes (demographics), and unlike the many predictive works we discuss in the next section, they test for significance of relationships between individual language features and outcomes. To contrast with our approach, we consider features and outcomes individually (i.e. an “L0 norm”), which we think is more ideal for our goals of explaining psychological variables (i.e. understanding openness by the words that correlate with it). For example, their method may throwout a word which is strongly predictive for only one outcome or which is collinear with other words, while we want to know all the words most-predictive for a given outcome. We also explore other types of <italic>open-vocabulary</italic> language features such as phrases and topics.</p>
<p>Similar language analyses also occurred in many fields outside of psychology or demographics <xref ref-type="bibr" rid="pone.0073791-OConnor1">[42]</xref>, <xref ref-type="bibr" rid="pone.0073791-Grimmer1">[43]</xref>. For example, Monroe et al. explored a variety of techniques that compare two frequencies of words – one number for each of two groups <xref ref-type="bibr" rid="pone.0073791-Monroe1">[44]</xref>. In particular, they explored frequencies across democratic versus republican speeches and settled on a Bayesian model with regularization and shrinkage based on priors of word use. Lastly, Gilbert finds words and phrases that distinguish communication up or down a power-hierarchy across 2044 Enron emails <xref ref-type="bibr" rid="pone.0073791-Gilbert1">[45]</xref>. They used penalized logistic regression to fit a single model using coefficients of each feature as their “power”; this produces a good single predictive model but also means words which are highly collinear with others will be missed (we run a separate regression for each word to avoid this).</p>
<p>Perhaps one of the most comprehensive language analysis surveys outside of psychology is that of Grimmer &amp; Stewart <xref ref-type="bibr" rid="pone.0073791-Grimmer1">[43]</xref>. They summarize how automated methods can inexpensively allow systematic analysis and inference from large political text collections, classifying types of analyses into a of hierarchy. Additionally, they provide cautionary advice; In relation to this work, they note that dictionary methods (such as the closed-vocabulary analyses discussed here) may signal something different when used in a new domain (for example ‘crude’ may be a negative word in student essays, but be neutral in energy industry reports: ‘crude oil’). For comprehensive surveys on text analyses across fields see Grimmer &amp; Stewart <xref ref-type="bibr" rid="pone.0073791-Grimmer1">[43]</xref>, O'Connor, Bamman, &amp; Smith <xref ref-type="bibr" rid="pone.0073791-OConnor1">[42]</xref>, and Tausczik &amp; Pennebaker <xref ref-type="bibr" rid="pone.0073791-Tausczik2">[46]</xref>.</p>
</sec><sec id="s2b">
<title>Predictive Models based on Language</title>
<p>In contrast with the works seeking to gain <italic>insights</italic> about psychological variables, research focused on <italic>predicting</italic> outcomes have embraced data-driven approaches. Such work uses open-vocabulary linguistic features in addition to <italic>a priori</italic> lexicon based features in predictive models for tasks such as stylistics/authorship attribution <xref ref-type="bibr" rid="pone.0073791-Holmes1">[47]</xref>–<xref ref-type="bibr" rid="pone.0073791-Stamatatos1">[49]</xref>, emotion prediction <xref ref-type="bibr" rid="pone.0073791-Alm1">[50]</xref>, <xref ref-type="bibr" rid="pone.0073791-Mihalcea1">[51]</xref>, interaction or flirting detection <xref ref-type="bibr" rid="pone.0073791-Jurafsky1">[52]</xref>, <xref ref-type="bibr" rid="pone.0073791-Ranganath1">[53]</xref>, or sentiment analysis <xref ref-type="bibr" rid="pone.0073791-Pang1">[54]</xref>–<xref ref-type="bibr" rid="pone.0073791-Baccianella1">[57]</xref>. In other works, ideologies of political figures (i.e. conservative to liberal) have been predicted based on language using supervised techniques <xref ref-type="bibr" rid="pone.0073791-Laver1">[58]</xref> or unsupervised inference of ideological space <xref ref-type="bibr" rid="pone.0073791-Monroe2">[59]</xref>, <xref ref-type="bibr" rid="pone.0073791-Slapin1">[60]</xref>. Sometimes these works note the highest weighted features, but with their goal being predictive accuracy, those features are not tested for significance and they usually are not the most individually distinguishing pieces of language. To elaborate, most approaches to prediction penalize the weights of words that are highly collinear with other words as they fit a single model per outcomes across all words. However, these highly collinear words which are suppressed, could have revealed important insights with an outcome. In other words, these predictive models answer the question “what is the best combination of words and weights to predict personality?” whereas we believe answering the following question is best for revealing new insights: “what words, controlled for gender and age, are individually most correlated with personality?”.</p>
<p>Recently, researchers have started looking at personality prediction. Early works in personality prediction used dictionary-based features such as <italic>LIWC</italic>. Argamon et al. (2005) noted that personality, as detected by categorical word use, was supportive for author attribution. They examined language use according to the traits of <italic>neuroticism</italic> and <italic>extraversion</italic> over approximately 2200 student essays, while focused on using function words for the prediction of gender <xref ref-type="bibr" rid="pone.0073791-Argamon5">[62]</xref>. Mairesse et al. used a variety of lexicon-based features to predict all Big-5 personality traits over approximately 2500 essays as well as 90 sets of individual spoken words <xref ref-type="bibr" rid="pone.0073791-Mairesse1">[63]</xref>, <xref ref-type="bibr" rid="pone.0073791-Mairesse2">[64]</xref>. As a first pass at predicting personality from language in Facebook, Golbeck used <italic>LIWC</italic> features over a sample of 167 Facebook volunteers as well as profile information and found limited success of a regression model <xref ref-type="bibr" rid="pone.0073791-Golbeck1">[65]</xref>. Similarly, Kaggle held a competition of personality prediction over Twitter messages, providing participants with language cues based on <italic>LIWC</italic> <xref ref-type="bibr" rid="pone.0073791-Sumner2">[66]</xref>. Results of the competition suggested personality is difficult to predict based on language in social media, but it is not clear whether such a conclusion would have been drawn had <italic>open-vocabulary</italic> language cues been supplied for prediction.</p>
<p>In the largest previous study of language and personality, Iacobelli, Gill, Nowson, and Oberlander attempted prediction of personality for 3,000 bloggers <xref ref-type="bibr" rid="pone.0073791-Iacobelli1">[67]</xref>. Not limited to categorical language they found open-vocabulary features, such as bigrams, to be better predictors than <italic>LIWC</italic> features. This motivates our exploration of open-vocabulary features for psychological insights, where we examine multi-word phrases (also called n-grams) as well as open-vocabulary category language in the form of automatically clustered groups of semantically related word (<italic>LDA topics</italic>, see “Linguistic Feature Extraction” in the “<xref ref-type="sec" rid="s3">Materials and Methods</xref>” section). Since the application of Iacobelli et al. 's work was content customization, they focused on prediction rather than exploration of language for psychological insight. Our much larger sample size lends itself well to more comprehensive exploratory results.</p>
<p>Similar studies have also been undertaken for age and gender prediction in social media. Because gender and age information is more readily available, these studies tend to be larger. Argamon et al. predicted gender and age over 19,320 bloggers <xref ref-type="bibr" rid="pone.0073791-Argamon1">[32]</xref>, while Burger et al. scaled up the gender prediction over 184,000 Twitter authors by using automatically guessed gender based-on gender-specific keywords in profiles. Most recently, Bamman et al. looked at gender as a function of language and social network statistics in twitter. They particularly looked at the characteristics of those whose gender was incorrectly predicted and found greater gender homophily in the social networks of such individuals <xref ref-type="bibr" rid="pone.0073791-Bamman1">[68]</xref>.</p>
<p>These past studies, mostly within the field of computer science or specifically computational linguistics, have focused on prediction for tasks such as content personalization or authorship attribution. In our work, predictive models of personality, gender, and age provide a quantitative means to compare various <italic>open-vocabulary</italic> sets of features with a <italic>closed-vocabulary</italic> set. Our primary concern is to explore the benefits of an <italic>open-vocabulary</italic> approach for gaining <italic>insights</italic>, a goal that is at least as import as prediction for psychosocial fields. Most works for gaining language-based insights in psychology are <italic>closed-vocabulary</italic> (for examples, see previous section), and while many works in computational linguistics are open-vocabulary, they rarely focus on insight. We introduce the term “open-vocabulary” to distinguish an approach like ours from previous approaches to gaining <italic>insight</italic>, and in order to encourage others seeking insights to consider similar approaches. “Differential language analysis” refers to the particular process, for which we are not aware of another name, we use in our <italic>open-vocabulary</italic> approach as depicted in <xref ref-type="fig" rid="pone-0073791-g001">Figure 1</xref>.</p>
<fig id="pone-0073791-g001" position="float" orientation="portrait"><object-id pub-id-type="doi">10.1371/journal.pone.0073791.g001</object-id><label>Figure 1</label><caption>
<title>The infrastructure of our differential language analysis.</title>
<p>1) <italic>Feature Extraction</italic>. Language use features include: <italic>(a) words and phrases</italic>: a sequence of 1 to 3 words found using an emoticon-aware tokenizer and a collocation filter (24,530 features) <italic>(b) topics</italic>: automatically derived groups of words for a single topic found using the Latent Dirichlet Allocation technique <xref ref-type="bibr" rid="pone.0073791-Blei1">[72]</xref>, <xref ref-type="bibr" rid="pone.0073791-McCallum1">[75]</xref> (500 features). 2) <italic>Correlational Analysis</italic>. We find the correlation (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e001" xlink:type="simple" /></inline-formula> of ordinary least square linear regression) between each language feature and each demographic or psychometric outcome. All relationships presented in this work are at least significant at a Bonferroni-corrected <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e002" xlink:type="simple" /></inline-formula> <xref ref-type="bibr" rid="pone.0073791-Dunn1">[76]</xref>. 3) <italic>Visualization</italic>. Graphical representation of correlational analysis output.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0073791.g001" position="float" xlink:type="simple" orientation="portrait" /></fig><fig id="pone-0073791-g002" position="float" orientation="portrait"><object-id pub-id-type="doi">10.1371/journal.pone.0073791.g002</object-id><label>Figure 2</label><caption>
<title>Correlation values of <italic>LIWC</italic> categories with gender, age, and the five factor model of personality.</title>
<p><xref ref-type="bibr" rid="pone.0073791-Newman1">[34]</xref> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e033" xlink:type="simple" /></inline-formula>: Effect size as Cohen's <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e034" xlink:type="simple" /></inline-formula> values from Newman et al. 's recent study of gender (positive is female, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e035" xlink:type="simple" /></inline-formula> not significant at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e036" xlink:type="simple" /></inline-formula>) <xref ref-type="bibr" rid="pone.0073791-Pennebaker4">[30]</xref>. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e037" xlink:type="simple" /></inline-formula>: Standardized linear regression coefficients adjusted for sex, writing/talking, and experimental condition from Pennebaker and Stone's study of age (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e038" xlink:type="simple" /></inline-formula> not significant at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e039" xlink:type="simple" /></inline-formula>) <xref ref-type="bibr" rid="pone.0073791-Yarkoni1">[27]</xref>. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e040" xlink:type="simple" /></inline-formula>: Spearman correlations values from Yarkoni's recent study of personality (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e041" xlink:type="simple" /></inline-formula> not significant at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e042" xlink:type="simple" /></inline-formula>). <bold>our</bold> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e043" xlink:type="simple" /></inline-formula>: Standardized multivariate regression coefficients adjusted for gender and age for this current study over Facebook (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e044" xlink:type="simple" /></inline-formula> =  not significant at Bonferroni-corrected <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e045" xlink:type="simple" /></inline-formula>).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0073791.g002" position="float" xlink:type="simple" orientation="portrait" /></fig></sec><sec id="s2c">
<title>Contributions</title>
<p>The contributions of this paper are as follows:</p>
<list list-type="bullet"><list-item>
<p>First, we present the largest study of personality and language use to date. With just under 75,000 authors, our study covers an order-of-magnitude more people and instances of language features than the next largest study (<xref ref-type="bibr" rid="pone.0073791-Yarkoni1">[27]</xref>). The size of our data enables qualitatively different analyses, including open vocabulary analysis, based on more comprehensive sets of language features such as <italic>phrases</italic> and automatically derived <italic>topics</italic>. Most prior studies used <italic>a priori</italic> language categories, presumably due in part to the sparse nature of words and their relatively small samples of people. With smaller data sets, it is difficult to find statistically significant differences in language use for anything but the most common words.</p>
</list-item><list-item>
<p>Our <italic>open-vocabulary</italic> analysis yields further insights into the behavioral residue of personality types beyond those from <italic>a priori</italic> word-category based approaches, giving unanticipated results (correlations between language and personality, gender, or age). For example, we make the novel discoveries that mentions of an assortment of social sports and life activities (such as <italic>basketball</italic>, <italic>snowboarding</italic>, <italic>church</italic>, <italic>meetings</italic>) correlate with <italic>emotional stability</italic>, and that <italic>introverts</italic> show an interest in Japanese media (such as <italic>anime</italic>, <italic>pokemon</italic>, <italic>manga</italic> and Japanese emoticons: ˆ_ˆ). Our inclusion of phrases in addition to words provided further insights (e.g. that males prefer to precede ‘girlfriend’ or ‘wife’ with the possessive ‘my’ significantly more than females do for ‘boyfriend’ or ‘husband’. Such correlations provide quantitative evidence for strong links between behavior, as revealed in language use, and psychosocial variables. In turn, these results suggest undertaking studies, such as directly measuring participation in activities in order to verify the link with emotional stability.</p>
</list-item><list-item>
<p>We demonstrate open-vocabulary features contain more information than <italic>a priori</italic> word-categories via their use in predictive models. We take model accuracy in out-of-sample prediction as a measure of information of the features provided to the model. Models built from words and phrases as well as those from automatically generated topics achieve significantly higher out-of-sample prediction accuracies than a standard lexica for each variable of interest (<italic>gender</italic>, <italic>age</italic>, and <italic>personality</italic>). Additionally, our prediction model for gender yielded state-of-the-art results for predictive models based entirely on language, yielding an out-of-sample accuracy of 91.9%.</p>
</list-item><list-item>
<p>We present a word cloud visualization which scales words by correlation (i.e., how well they predict the given psychological variable) rather than simply scaling by frequency. Since we find thousands of significantly correlated words, visualization is key, and our <italic>differential</italic> word clouds provide a comprehensive view of our results (e.g. see <xref ref-type="fig" rid="pone-0073791-g003">Figure 3</xref>).</p>
</list-item><list-item>
<p>Lastly, we offer our comprehensive <italic>word</italic>, <italic>phrase</italic>, and <italic>topic</italic> correlation data for future research experiments (see: wwbp.org).</p>
</list-item></list>
<fig id="pone-0073791-g003" position="float" orientation="portrait"><object-id pub-id-type="doi">10.1371/journal.pone.0073791.g003</object-id><label>Figure 3</label><caption>
<title>Words, phrases, and topics most highly distinguishing females and males.</title>
<p>Female language features are shown on top while males below. Size of the word indicates the strength of the correlation; color indicates relative frequency of usage. Underscores (_) connect words of multiword phrases. <italic>Words and phrases</italic> are in the center; <italic>topics</italic>, represented as the 15 most prevalent words, surround. (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e047" xlink:type="simple" /></inline-formula>: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e048" xlink:type="simple" /></inline-formula> females and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e049" xlink:type="simple" /></inline-formula> males; correlations adjusted for age; Bonferroni-corrected <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e050" xlink:type="simple" /></inline-formula>).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0073791.g003" position="float" xlink:type="simple" orientation="portrait" /></fig></sec></sec><sec id="s3" sec-type="materials|methods">
<title>Materials and Methods</title>
<sec id="s3a">
<title>Ethics Statement</title>
<p>All research procedures were approved by the University of Pennsylvania Institutional Review Board. Volunteers agreed to written informed consent.</p>
<p>In seeking insights from language use about personality, gender, and age, we explore two approaches. The first approach, serving as a replication of the past analyses, counts word usage over manually created <italic>a priori</italic> word-category lexica. The second approach, termed <italic>DLA</italic>, serves as out main method and is <italic>open-vocabulary</italic> – the words and clusters of words analyzed are determined by the data itself.</p>
</sec><sec id="s3b">
<title>Closed Vocabulary: Word-Category Lexica</title>
<p>A common method for linking language with psychological variables involves counting words belonging to manually-created categories of language. Sometimes referred to as the <italic>word-count</italic> approach, one counts how often words in a given category are used by an individual, the percentage of the participants' words which are from the given category:<disp-formula id="pone.0073791.e003"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0073791.e003" xlink:type="simple" orientation="portrait" /></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e004" xlink:type="simple" /></inline-formula> is the number of the times the participant mentions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e005" xlink:type="simple" /></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e006" xlink:type="simple" /></inline-formula> is the set of all words mentioned by the subject.</p>
<p>We use ordinary least squares regression to link word categories with author attributes, fitting a linear function between explanatory variables (<italic>LIWC</italic> categories) and dependent variables (such as a trait of personality, e.g. extraversion). The coefficient of the target explanatory variable (often referred to as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e007" xlink:type="simple" /></inline-formula>) is taken as the strength of relationship. Including other variables allows us to adjust for covariates such as gender and age to provide the unique effect of a given language feature on each psychosocial variable.</p>
</sec><sec id="s3c">
<title>Open Vocabulary: Differential Language Analysis</title>
<p>Our technique, <italic>differential language analysis</italic> (<italic>DLA</italic>), is based on three key characteristics. It is</p>
<list list-type="order"><list-item>
<p><italic>Open-vocabulary</italic> – it is not limited to predefined word lists. Rather, linguistic features including words, phrases, and topics (sets of semantically related words) are automatically determined from the texts. (I.e., it is “data-driven”.) This means <italic>DLA</italic> is classified as a type of open-vocabulary approach.</p>
</list-item><list-item>
<p><italic>Discriminating</italic> – it finds key linguistic features that distinguish psychological and demographic attributes, using stringent significance tests.</p>
</list-item><list-item>
<p><italic>Simple</italic> – it uses simple, fast, and readily accepted statistical techniques.</p>
</list-item></list>
<p>We depict the components of this approach in <xref ref-type="fig" rid="pone-0073791-g001">Figure 1</xref>, and describe the three steps: 1) linguistic feature extraction, 2) correlational analysis, and 3) visualization in the following sections.</p>
<sec id="s3c1">
<title>1. Linguistic Feature Extraction</title>
<p>We examined two types of linguistic features: a) <italic>words and phrases</italic>, and b) <italic>topics</italic>. <italic>Words and phrases</italic> consisted of sequences of 1 to 3 words (often referred to as ‘n-grams’ of size 1 to 3). What constitutes a word is determined using a tokenizer, which splits sentences into tokens (“words”). We built an emoticon-aware tokenizer on top of Pott's “happyfuntokenizer” allowing us to capture emoticons like ‘<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e008" xlink:type="simple" /></inline-formula>3’(a heart) or ‘:-)’ (a smile), which most tokenizers incorrectly divide up as separate pieces of punctuation. When extracting phrases, we keep only those sequences of words with high informative value according to pointwise mutual information (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e009" xlink:type="simple" /></inline-formula>) <xref ref-type="bibr" rid="pone.0073791-Church1">[69]</xref>, <xref ref-type="bibr" rid="pone.0073791-Lin1">[70]</xref>, a ratio of the joint-probability to the independent probability of observing the phrase:<disp-formula id="pone.0073791.e010"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0073791.e010" xlink:type="simple" orientation="portrait" /></disp-formula></p>
<p>In practice, we kept phrases with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e011" xlink:type="simple" /></inline-formula> values greater than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e012" xlink:type="simple" /></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e013" xlink:type="simple" /></inline-formula> is the number of words contained in the phrase, ensuring that phrases we do keep are informative parts of speech and not just accidental juxtapositions. All word and phrase counts are normalized by each subject's total word use (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e014" xlink:type="simple" /></inline-formula>), and we apply the Anscombe transformation <xref ref-type="bibr" rid="pone.0073791-Anscombe1">[71]</xref> to the normalized values for variance stabilization (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e015" xlink:type="simple" /></inline-formula>):<disp-formula id="pone.0073791.e016"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0073791.e016" xlink:type="simple" orientation="portrait" /></disp-formula><disp-formula id="pone.0073791.e017"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0073791.e017" xlink:type="simple" orientation="portrait" /></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e018" xlink:type="simple" /></inline-formula> returns a list of all words and phrases used by that subject. These Anscombe transformed “relative frequencies” of words or phrases (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e019" xlink:type="simple" /></inline-formula>) are then used as the independent variables in all our analyses. Lastly, we restrict our analysis to those words and phrases which are used by at least 1% of our subjects, keeping the focus on common language.</p>
<p>The second type of linguistic feature, <italic>topics</italic>, consists of word clusters created using Latent Dirichlet Allocation (LDA) <xref ref-type="bibr" rid="pone.0073791-Blei1">[72]</xref>, <xref ref-type="bibr" rid="pone.0073791-Steyvers1">[73]</xref>. The LDA generative model assumes that documents (i.e. Facebook messages) contain a combination of topics, and that topics are a distribution of words; since the words in a document are known, the latent variable of topics can be estimated through Gibbs sampling <xref ref-type="bibr" rid="pone.0073791-Gelfand1">[74]</xref>. We use an implementation of the LDA algorithm provided by the Mallet package <xref ref-type="bibr" rid="pone.0073791-McCallum1">[75]</xref>, adjusting one parameter (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e020" xlink:type="simple" /></inline-formula>) to favor fewer topics per document, since individual Facebook status updates tend to contain fewer topics than the typical documents (newspaper or encyclopedia articles) to which LDA is applied. All other parameters were kept at their default. An example of such a model is the following sets of words (<italic>tuesday, monday, wednesday, friday, thursday, week, sunday, saturday</italic>) which clusters together days of the week purely by exploiting their similar distributional properties across messages. We produced the 2000 topics shown in <xref ref-type="supplementary-material" rid="pone.0073791.s003">Table S1</xref> as well as on our website.</p>
<p>To use topics as features, we find the probability of a subject's use of each <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e021" xlink:type="simple" /></inline-formula>:<disp-formula id="pone.0073791.e022"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0073791.e022" xlink:type="simple" orientation="portrait" /></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e023" xlink:type="simple" /></inline-formula> is the normalized word use by that subject and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e024" xlink:type="simple" /></inline-formula> is the probability of the topic given the word (a value provided from the LDA procedure). The prevalence of a word in a topic is given by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e025" xlink:type="simple" /></inline-formula>, and is used to order the words within a topic when displayed.</p>
</sec><sec id="s3c2">
<title>2. Correlational Analysis</title>
<p>Similar to word categories, distinguishing open-vocabulary words, phrases, and topics can be identified using ordinary least squares regression. We again take the coefficient of the target explanatory variable as its correlation strength, and we include other variables (e.g. age and gender) as covariates to get the unique effect of the target explanatory variable. Since we explore many features at once, we consider coefficients significant if they are less than a Bonferroni-corrected <xref ref-type="bibr" rid="pone.0073791-Dunn1">[76]</xref> two-tailed <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e026" xlink:type="simple" /></inline-formula> of 0.001. (I.e., when examining 20,000 features, a passing p-value is less than 0.001 divided by 20,000 which is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e027" xlink:type="simple" /></inline-formula>).</p>
<p>Our correlational analysis produces a comprehensive list of the most distinguishing language features for any given attribute, <italic>words, phrases,</italic> or <italic>topics</italic> which maximally discriminate a given target variables. For example, when we correlate the target variables geographic elevation with language features (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e028" xlink:type="simple" /></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e029" xlink:type="simple" /></inline-formula>, adjusted for gender and age), we find ‘beach’ the most distinguishing feature for low elevation localities, and ‘the mountains’ to be among the most distinguishing features for high elevation localities, (i.e., people in low elevations talk about the beach more, whereas people at high elevations talk about the mountains more). Similarly, we find the most distinguishing topics to be <italic>(beach, sand, sun, water, waves, ocean, surf, sea, toes, sandy, surfing, beaches, sunset, Florida, Virginia)</italic> for low elevations and <italic>(Colorado, heading, headed, leaving, Denver, Kansas, City, Springs, Oklahoma, trip, moving, Iowa, KC, Utah, bound)</italic> for high elevations. Others have looked at geographic location <xref ref-type="bibr" rid="pone.0073791-Eisenstein2">[77]</xref>.</p>
</sec><sec id="s3c3">
<title>3. Visualization</title>
<p>An analysis over tens of thousands of language features and multiple dimensions results in hundreds of thousands of statistically significant correlations. Visualization is thus critical for their interpretation. We use word clouds <xref ref-type="bibr" rid="pone.0073791-Wordle1">[78]</xref> to intuitively summarize our results. Unlike most word clouds, which scale word size by their frequency, we scale word size according to the strength of the correlation of the word with the demographic or psychological measurement of interest, and we use color to represent frequency over all subjects; that is, larger words indicate stronger correlations, and darker colors indicate more frequently used words. This provides a clear picture of which words and phrases are most discriminating while not losing track of which ones are the most frequent. Word clouds scaled by frequency are often used to summarize news, a practice that has been critiqued for inaccurately representing articles <xref ref-type="bibr" rid="pone.0073791-Harris1">[79]</xref>. Here, we believe the word cloud is an appropriate visualization because the individual words and phrases we depict in it are the actual results we wish to summarize. Further, scaling by correlation coefficient rather than frequency gives clouds that distinguish a given outcome.</p>
<p>Word clouds can also used to represent distinguishing topics. In this case, the size of the word within the topic represents its prevalence among the cluster of words making up the topic. We use the 6 most distinguishing topics and place them on the perimeter of the word clouds for <italic>words and phrases</italic>. This way, a single figure gives a comprehensive view of the most distinguishing words, phrases, and topics for any given variables of interest. See <xref ref-type="fig" rid="pone-0073791-g003">Figure 3</xref> for an example.</p>
<p>To reduce the redundancy of results, we automatically prune language features containing information already provided by a feature with higher correlation. First, we sort language features in order of their correlation with a target variable (such as a personality trait). Then, for phrases, we use frequency as a proxy for informative value <xref ref-type="bibr" rid="pone.0073791-Resnik1">[80]</xref>, and only include additional phrases if they contain more informative words than previously included phrases with matching words. For example, consider the phrases ‘day’, ‘beautiful day’, and ‘the day’, listed in order of correlation from greatest to least; ‘Beautiful day’ would be kept, because ‘beautiful’ is less frequent than ‘day’ (i.e., it is adding informative value), while ‘the day’ would be dropped because ‘the’ is more frequent than ‘day’ (thus it is not contributing more information than we get from ‘day’). We do a similar pruning for topics: A lower-ranking topic is not displayed if more than 25% of its top 15 words are also contained in the top 15 words of a higher ranking topic. These discarded relationships are still statistically significant, but removing them provides more room in the visualizations for other significant results, making the visualization as a whole more meaningful.</p>
<p>Word clouds allow one to easily view the features most correlated with polar outcomes; we use other visualizations to display the variation of correlation of language features with continuous or ordinal dependent variables such as age. A standard time-series plot works well, where the horizontal axis is the dependent variable and the vertical axis represents the standard score of the values produced from feature extraction. When plotting language as a function of age, we fit first-order LOESS regression lines <xref ref-type="bibr" rid="pone.0073791-Cleveland1">[81]</xref> to the age as the x-axis data and standardized frequency as the y-axis data over all users. We are able to adjust for gender in the regression model by including it as a covariate when training the LOESS model and then using a neutral gender value when plotting.</p>
</sec></sec><sec id="s3d">
<title>Data Set: Facebook Status Updates</title>
<p>Our complete dataset consists of approximately 19 million Facebook status updates written by 136,000 participants. Participants volunteered to share their status updates as part of the <italic>My Personality</italic> application, where they also took a variety of questionnaires <xref ref-type="bibr" rid="pone.0073791-Kosinski1">[12]</xref>. We restrict our analysis to those Facebook users meeting certain criteria: They must indicate English as a primary language, have written at least 1,000 words in their status updates, be less than 65 years (to avoid the non-representative sample above 65), and indicate both gender and age (for use as controls). This resulted in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e030" xlink:type="simple" /></inline-formula> volunteers, writing a total of 309 million words (700 million feature instances of words, phrases, and topics) across 15.4 million status updates. From this sample each person wrote an average of 4,129 words over 206 status updates, and thus 20 words per update. Depending on the target variable, this number slightly varies as indicated in the caption of each result.</p>
<p>The personality scores are based on the International Personality Item Pool proxy for the NEO Personality Inventory Revised (NEO-PI-R) <xref ref-type="bibr" rid="pone.0073791-McCrae1">[14]</xref>, <xref ref-type="bibr" rid="pone.0073791-CostaJr1">[82]</xref>. Participants could take 20 to 100 item versions of the questionnaire, with a retest reliability of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e031" xlink:type="simple" /></inline-formula> <xref ref-type="bibr" rid="pone.0073791-Kosinski1">[12]</xref>. With the addition of gender and age variables, this resulted in seven total dependent variables studied in this work, which are depicted in <xref ref-type="table" rid="pone-0073791-t001">Table 1</xref> along with summary statistics. Personality distributions are quite typical with means near zero and standard deviations near 1. The statuses ranged over 34 months, from January 2009 through October 2011. Previously, profile information (i.e. network metrics, relationship status) from users in this dataset have been linked with personality <xref ref-type="bibr" rid="pone.0073791-Bachrach1">[83]</xref>, but this is the first use of its status updates.</p>
<table-wrap id="pone-0073791-t001" position="float" orientation="portrait"><object-id pub-id-type="doi">10.1371/journal.pone.0073791.t001</object-id><label>Table 1</label><caption>
<title>Summary statistics for gender, age, and the five factor model of personality.</title>
</caption><alternatives><graphic id="pone-0073791-t001-1" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0073791.t001" xlink:type="simple" orientation="portrait" />
<table><colgroup span="1"><col align="left" span="1" /><col align="center" span="1" /><col align="center" span="1" /><col align="center" span="1" /><col align="center" span="1" /></colgroup>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1" />
<td align="left" rowspan="1" colspan="1"><italic>N</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>mean</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>standard deviation</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>skewness</italic></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Gender</bold></td>
<td align="left" rowspan="1" colspan="1">74859</td>
<td align="left" rowspan="1" colspan="1">0.62</td>
<td align="left" rowspan="1" colspan="1">0.49</td>
<td align="left" rowspan="1" colspan="1">−0.50</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Age</bold></td>
<td align="left" rowspan="1" colspan="1">74859</td>
<td align="left" rowspan="1" colspan="1">23.43</td>
<td align="left" rowspan="1" colspan="1">8.96</td>
<td align="left" rowspan="1" colspan="1">1.77</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Extraversion</bold></td>
<td align="left" rowspan="1" colspan="1">72709</td>
<td align="left" rowspan="1" colspan="1">−0.07</td>
<td align="left" rowspan="1" colspan="1">1.01</td>
<td align="left" rowspan="1" colspan="1">−0.34</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Agreeableness</bold></td>
<td align="left" rowspan="1" colspan="1">72772</td>
<td align="left" rowspan="1" colspan="1">0.03</td>
<td align="left" rowspan="1" colspan="1">1.00</td>
<td align="left" rowspan="1" colspan="1">−0.40</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Conscientiousness</bold></td>
<td align="left" rowspan="1" colspan="1">72781</td>
<td align="left" rowspan="1" colspan="1">−0.04</td>
<td align="left" rowspan="1" colspan="1">1.01</td>
<td align="left" rowspan="1" colspan="1">−0.09</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Neuroticism</bold></td>
<td align="left" rowspan="1" colspan="1">71968</td>
<td align="left" rowspan="1" colspan="1">0.14</td>
<td align="left" rowspan="1" colspan="1">1.04</td>
<td align="left" rowspan="1" colspan="1">−0.21</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Openness</bold></td>
<td align="left" rowspan="1" colspan="1">72809</td>
<td align="left" rowspan="1" colspan="1">0.12</td>
<td align="left" rowspan="1" colspan="1">0.97</td>
<td align="left" rowspan="1" colspan="1">−0.48</td>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt101"><label /><p>These represent the seven dependent variables studied in this work. Gender ranged from 0 (male) to 1(female). Age ranged from 13 to 65. Personality questionnaires produce values along a standardized continuum.</p></fn></table-wrap-foot></table-wrap></sec></sec><sec id="s4">
<title>Results</title>
<p>Results of our analyses over gender, age, and personality are presented below. As a baseline, we first replicate the commonly used <italic>LIWC</italic> analysis on our data set. We then present our main results, the output of our method, <italic>DLA</italic>. Lastly, we explore empirical evidence that <italic>open-vocabulary</italic> features provide more information than those from an <italic>a priori</italic> lexicon through use in a predictive model.</p>
<sec id="s4a">
<title>Closed Vocabulary</title>
<p><xref ref-type="fig" rid="pone-0073791-g002">Figure 2</xref> shows the results of applying the <italic>LIWC</italic> lexicon to our dataset, along side-by-side with the most comprehensive previous studies we could find for <italic>gender</italic>, <italic>age</italic>. and <italic>personality</italic> <xref ref-type="bibr" rid="pone.0073791-Yarkoni1">[27]</xref>, <xref ref-type="bibr" rid="pone.0073791-Pennebaker4">[30]</xref>, <xref ref-type="bibr" rid="pone.0073791-Newman1">[34]</xref>. In our case, correlation results are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e032" xlink:type="simple" /></inline-formula> values from an ordinary least squares linear regression where we can adjust for gender and age to give the unique effect of the target variable. One should keep in mind that it is often found that effect sizes tend to be relatively smaller as sample sizes increase and become more stable <xref ref-type="bibr" rid="pone.0073791-Sterne1">[84]</xref>.</p>
<p>Even though the previous studies listed did not look at Facebook, a majority of the correlations we find agree in direction. Some of the largest correlations emerge for the LIWC <italic>articles</italic> category, which consists of determiners like ‘the’, 'a’, ‘an’ and serves as a proxy for the use of more nouns. Articles are highly predictive of males, being older, and <italic>openness</italic>. As a content-related language variable, the <italic>anger</italic> category also proved highly predictive for <italic>males</italic> as well as younger individuals, those low in <italic>agreeableness</italic> and <italic>conscientiousness</italic>, and high in <italic>neuroticism</italic>. <italic>Openness</italic> had the least agreement with the comparison study; roughly half of our results were in the opposite direction from the prior work. This is not too surprising since <italic>openness</italic> exhibits the most variation across conditions of other studies (for examples, see <xref ref-type="bibr" rid="pone.0073791-Sumner1">[25]</xref>, <xref ref-type="bibr" rid="pone.0073791-Yarkoni1">[27]</xref>, <xref ref-type="bibr" rid="pone.0073791-Golbeck1">[65]</xref>), and its component traits are most loosely related <xref ref-type="bibr" rid="pone.0073791-McCrae2">[85]</xref>.</p>
</sec><sec id="s4b">
<title>Open Vocabulary</title>
<p>Our <italic>DLA</italic> method identifies the most distinguishing language features (<italic>words, phrases</italic>: a sequence of 1 to 3 words, or <italic>topics</italic>: a cluster of semantically related words) for any given attribute. Results progress from a one variable proof of concept (gender), to the multiple variables representing age groups, and finally to all 5 dimensions of personality.</p>
<sec id="s4b1">
<title>Language of Gender</title>
<p>Gender provides a familiar and easy to understand proof of concept for open-vocabulary analysis. <xref ref-type="fig" rid="pone-0073791-g003">Figure 3</xref> presents word clouds from age-adjusted gender correlations. We scale word size according to the strength of the relation and we use color to represent overall frequency; that is, larger words indicate stronger correlations, and darker colors indicate frequently used words. For the <italic>topics</italic>, groups of semantically-related words, the size indicate the relative prevalence of the word within the cluster as defined in the methods section. All results are significant at Bonferroni-corrected <xref ref-type="bibr" rid="pone.0073791-Dunn1">[76]</xref> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e046" xlink:type="simple" /></inline-formula>.</p>
<p>Many strong results emerging from our analysis align with our <italic>LIWC</italic> results and past studies of gender. For example, females used more emotion words <xref ref-type="bibr" rid="pone.0073791-Mulac1">[86]</xref>, <xref ref-type="bibr" rid="pone.0073791-Thomson1">[87]</xref> (e.g., ‘excited’), and first-person singulars <xref ref-type="bibr" rid="pone.0073791-Mehl2">[88]</xref>, and they mention more psychological and social processes <xref ref-type="bibr" rid="pone.0073791-Newman1">[34]</xref> (e.g., ‘love you’ and ‘<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e051" xlink:type="simple" /></inline-formula>3’ –a heart). Males used more swear words, object references (e.g., ‘xbox’ and swear words) <xref ref-type="bibr" rid="pone.0073791-Newman1">[34]</xref>, <xref ref-type="bibr" rid="pone.0073791-Mulac2">[89]</xref>.</p>
<p>Other results of ours contradicted past studies, which were based upon significantly smaller sample sizes than ours. For example, in 100 bloggers Huffaker et al. <xref ref-type="bibr" rid="pone.0073791-Huffaker1">[39]</xref> found males use more emoticons than females. We calculated power analyses to determine the sample size needed to confidently find such significant results. Since the Bonferonni-correction we use elsewhere in this work is overly stringent (i.e. makes it harder than necessary to pass significance tests), for this result we applied the Benjamini-Hochberg false discovery rate procedure for multiple hypothesis testing <xref ref-type="bibr" rid="pone.0073791-Benjamini1">[90]</xref>. Rerunning our language of gender analysis on reduced random samples of our subjects resulted in the following number of significant correlations (Benjamini-Hochberg tested <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e052" xlink:type="simple" /></inline-formula>): 50 subjects: 0 significant correlations, 500 subjects: 7 correlations; 5,000 subjects: 1,489 correlations; 50,000 subjects: 13,152 correlations (more detailed results of power analyses across gender, age, and personality can be found in <xref ref-type="supplementary-material" rid="pone.0073791.s001">Figure S1</xref>). Thus, traditional study sample sizes, which are closer to 50 or 500, are not powerful enough to do data-driven DLA over individual words.</p>
<p>One might also draw insights based on the gender results. For example, we noticed ‘my wife’ and ‘my girlfriend’ emerged as strongly correlated in the male results, while simply ‘husband’ and ‘boyfriend’ were most predictive for females. Investigating the frequency data revealed that males did in fact precede such references to their opposite-sex partner with ‘my’ significantly more often than females. On the other hand, females were more likely to precede ‘husband’ or ‘boyfriend’ with ‘her’ or ‘amazing’ and a greater variety of words, which is why ‘my husband’ was not more predictive than ‘husband’ alone. Furthermore, this suggests the male preference for the possessive ‘my’ is at least partially due to a lack of talking about others' partners.</p>
</sec><sec id="s4b2">
<title>Language of Age</title>
<p><xref ref-type="fig" rid="pone-0073791-g004">Figure 4</xref> shows the word cloud (center) and most discriminating topics (surrounding) for four age buckets chosen with regard to the distribution of ages in our sample (Facebook has many more young people). We see clear distinctions, such as use of slang, emoticons, and Internet speak in the youngest group (e.g. ':)’, ‘idk’, and a couple <italic>Internet speak</italic> topics) or work appearing in the 23 to 29 age group (e.g. ‘at work’, ‘new job’, as a <italic>job position</italic> topic). We also find subtle changes of topics progressing from one age group to the next. For example, we see a <italic>school</italic> related topic for 13 to 18 year olds (e.g. ‘school’, ‘homework’, ‘ugh’), while we see a <italic>college</italic> related topic for 19 to 22 year olds (e.g. ‘semester’, ‘college’, ‘register’). Additionally, consider the <italic>drunk</italic> topic (e.g. ‘drunk’, ‘hangover’, ‘wasted’) that appears for 19 to 22 year olds and a more reserved <italic>beer</italic> topic (e.g. ‘beer’, ‘drinking’, ‘ale’) for 23 to 29 year olds.</p>
<fig id="pone-0073791-g004" position="float" orientation="portrait"><object-id pub-id-type="doi">10.1371/journal.pone.0073791.g004</object-id><label>Figure 4</label><caption>
<title>Words, phrases, and topics most distinguishing subjects aged <italic>13 to 18</italic>, <italic>19 to 22</italic>, <italic>23 to 29</italic>, and <italic>30 to 65</italic>.</title>
<p>Ordered from top to bottom: <italic>13 to 18 19 to 22 23 to 29</italic>, and <italic>30 to 65</italic>. <italic>Words and phrases</italic> are in the center; <italic>topics</italic>, represented as the 15 most prevalent words, surround. (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e053" xlink:type="simple" /></inline-formula>; correlations adjusted for gender; Bonferroni-corrected <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e054" xlink:type="simple" /></inline-formula>).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0073791.g004" position="float" xlink:type="simple" orientation="portrait" /></fig>
<p>In general, we find a progression of school, college, work, and family when looking at the predominant topics across all age groups. <italic>DLA</italic> may be valuable for the generation of hypotheses about life span developmental age differences. <xref ref-type="fig" rid="pone-0073791-g005">Figure 5A</xref> shows the relative frequency of the most discriminating topic for each age group as a function of age. Typical concerns peak at different ages, with the topic concerning relationships (e.g. ‘son’, ‘daughter’, ‘father’, ‘mother’) continuously increasing across life span. On a similar note, <xref ref-type="fig" rid="pone-0073791-g005">Figure 5C</xref> shows ‘we’ increases approximately linearly after the age of 22, whereas ‘I’ monotonically decreases. We take this as a proxy for social integration <xref ref-type="bibr" rid="pone.0073791-Pennebaker2">[19]</xref>, suggesting the increasing importance of friendships and relationships as people age. <xref ref-type="fig" rid="pone-0073791-g005">Figure 5B</xref> reinforces this hypothesis by presenting a similar pattern based on other social topics. One limitation of our dataset is the rarity of older individuals using social media; we look forward to a time in which we can track fine-grained language differences across the entire lifespan.</p>
<fig id="pone-0073791-g005" position="float" orientation="portrait"><object-id pub-id-type="doi">10.1371/journal.pone.0073791.g005</object-id><label>Figure 5</label><caption>
<title>Standardized frequency of topics and words across age.</title>
<p><bold>A</bold>. Standardized frequency for the best topic for each of the 4 age groups. Grey vertical lines divide groups: 13 to 18 (black: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e055" xlink:type="simple" /></inline-formula> out of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e056" xlink:type="simple" /></inline-formula>), 19 to 22 (green: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e057" xlink:type="simple" /></inline-formula>), 23 to 29 (blue: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e058" xlink:type="simple" /></inline-formula>), and 30+ (red: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e059" xlink:type="simple" /></inline-formula>). Lines are fit from first-order LOESS regression <xref ref-type="bibr" rid="pone.0073791-Cleveland1">[81]</xref> controlled for gender. <bold>B</bold>. Standardized frequency of social topic use across age. <bold>C</bold>. Standardized ‘I’, ‘we’ frequencies across age.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0073791.g005" position="float" xlink:type="simple" orientation="portrait" /></fig></sec><sec id="s4b3">
<title>Language of Personality</title>
<p>We created age and gender-adjusted word clouds for each personality factor based on around 72 thousand participants with at least 1,000 words across their Facebook status updates, who took a Big Five questionnaire <xref ref-type="bibr" rid="pone.0073791-Goldberg2">[91]</xref>.</p>
<p><xref ref-type="fig" rid="pone-0073791-g006">Figure 6</xref> shows word clouds for extraversion and neuroticism. (See <xref ref-type="supplementary-material" rid="pone.0073791.s002">Figure S2</xref> for openness, conscientiousness, and agreeableness.) The dominant words in each cluster were consistent with prior lexical and questionnaire work <xref ref-type="bibr" rid="pone.0073791-McCrae1">[14]</xref>. For example, extraverts were more likely to mention social words such as ‘party’, ‘love you’, ‘boys’, and ‘ladies’, whereas introverts were more likely to mention words related to solitary activities such as ‘computer’, ‘Internet’, and ‘reading’. In the openness cloud, words such as ‘music’, ‘art’, and ‘writing’ (i.e., creativity), and ‘dream’, ‘universe’, and ‘soul’ (i.e., imagination) were discriminating <xref ref-type="bibr" rid="pone.0073791-McCrae2">[85]</xref>.</p>
<fig id="pone-0073791-g006" position="float" orientation="portrait"><object-id pub-id-type="doi">10.1371/journal.pone.0073791.g006</object-id><label>Figure 6</label><caption>
<title>Words, phrases, and topics most distinguishing <italic>extraversion</italic> from <italic>introversion</italic> and <italic>neuroticism</italic> from <italic>emotional stability</italic>.</title>
<p><bold>A</bold>. Language of extraversion (left, e.g., ‘party’) and introversion (right, e.g., ‘computer’); <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e060" xlink:type="simple" /></inline-formula>. <bold>B</bold>. Language distinguishing neuroticism (left, e.g. ‘hate’) from emotional stability (right, e.g., ‘blessed’); <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e061" xlink:type="simple" /></inline-formula> (adjusted for age and gender, Bonferroni-corrected <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e062" xlink:type="simple" /></inline-formula>). Figure S8 contains results for <italic>openness</italic>, <italic>conscientiousness</italic>, and <italic>agreeableness</italic>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0073791.g006" position="float" xlink:type="simple" orientation="portrait" /></fig>
<p>Topics were also found reflecting similar concepts as the words, some of which would not have been captured with <italic>LIWC</italic>. For example, although <italic>LIWC</italic> has socially related categories, it does not contain a <italic>party</italic> topic, which emerges as a key distinguishing feature for extraverts. Topics related to other types of social events are listed elsewhere, such as a sports topic for low neuroticism (emotional stability). Additionally, <xref ref-type="fig" rid="pone-0073791-g006">Figure 6</xref> shows the advantage of having phrases in the analysis to get clearer signal: e.g. people high in neuroticism mentioned ‘sick of’, and not just ‘sick’.</p>
<p>While many of our results confirm previous research, demonstrating the instrument's face validity, our word clouds also suggest new hypotheses. For example, <xref ref-type="fig" rid="pone-0073791-g006">Figure 6</xref> (bottom-right) shows language related to emotional stability (low neuroticism). Emotionally stable individuals wrote about enjoyable social activities that may foster greater emotional stability, such as ‘sports’, ‘vacation’, ‘beach’, ‘church’, ‘team’, and a <italic>family time</italic> topic. Additionally, results suggest that introverts are interested in Japanese media (e.g. ‘anime’, ‘manga’, ‘japanese’, Japanese style emoticons: ˆ_ˆ, and an <italic>anime</italic> topic) and that those low in <italic>openness</italic> drive the use of shorthands in social media (e.g. ‘2day’, ‘ur’, ‘every 1’). Although these are only language correlations, they show how <italic>open-vocabulary</italic> analyses can illuminate areas to explore further.</p>
</sec></sec><sec id="s4c">
<title>Predictive Evaluation</title>
<p>Here we present a quantitative evaluation of open-vocabulary and closed vocabulary language features. Although we have thus far presented subjective evidence that open-vocabulary features contribute more information, we hypothesize empirically that the inclusion of open-vocabulary features leads to prediction accuracies above and beyond that of closed-vocabulary. We randomly sampled 25% of our participants as test data, and used the remaining 75% as training data to build our predictive models.</p>
<p>We use a linear support vector machine (<italic>SVM</italic>) <xref ref-type="bibr" rid="pone.0073791-Fan1">[92]</xref> for classifying the binary variable of gender, and ridge regression <xref ref-type="bibr" rid="pone.0073791-Hoerl1">[93]</xref> for predicting age and each factor of personality. Features were first run through principal component analysis to reduce the feature dimension to half of the number of users. Both SVM classification and ridge regression utilize a regularization parameter, which we set by validation over the training set (we defined a small validation set of 10% of the training set which we tested various regularization parameters over while fitting the model to the other 90% of the training set in order to select the best parameter). Thus, the predictive model is created without any outcome information outside of the training data, making the test data an out-of-sample evaluation.</p>
<p>As open-vocabulary features, we use the same units of language as <italic>DLA</italic>: <italic>words and phrases</italic> (n-grams of size 1 to 3, passing a collocation filter) and <italic>topics</italic>. These features are outlined precisely under the “Linguistic Feature Extraction” section presented earlier. As explained in that section, we use Anscombe transformed relative frequencies of <italic>words and phrases</italic> and the conditional probability of a <italic>topic</italic> given a subject. For closed vocabulary features, we use the <italic>LIWC</italic> categories of language calculated as the relative frequency of a user mentioning a word in the category given their total word usage. We do not provide our models with anything other than these language usage features (independent variables) for prediction, and we use usage of all features (not just those passing significance tests from <italic>DLA</italic>).</p>
<p>As shown in <xref ref-type="table" rid="pone-0073791-t002">Table 2</xref>, we see that models created with <italic>open vocabulary</italic> features significantly (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e063" xlink:type="simple" /></inline-formula>) outperformed those created based on <italic>LIWC</italic> features. The <italic>topics</italic> results are of particular interest, because these automatically clustered word-category lexica were not created with any human or psychological data – only knowing what words occurred in messages together. Furthermore, we see that a model which includes <italic>LIWC</italic> features on top of the <italic>open-vocabulary words</italic>, <italic>phrases</italic>, and <italic>topics</italic> does not result in any improvement suggesting that the open-vocabulary features are able to capture predictive information which fully supersedes <italic>LIWC</italic>.</p>
<table-wrap id="pone-0073791-t002" position="float" orientation="portrait"><object-id pub-id-type="doi">10.1371/journal.pone.0073791.t002</object-id><label>Table 2</label><caption>
<title>Comparison of <italic>LIWC</italic> and <italic>open-vocabulary</italic> features within predictive models of gender, age, and personality.</title>
</caption><alternatives><graphic id="pone-0073791-t002-2" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0073791.t002" xlink:type="simple" orientation="portrait" />
<table><colgroup span="1"><col align="left" span="1" /><col align="center" span="1" /><col align="center" span="1" /><col align="center" span="1" /><col align="center" span="1" /><col align="center" span="1" /><col align="center" span="1" /><col align="center" span="1" /></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1" />
<td align="left" rowspan="1" colspan="1">Gender</td>
<td align="left" rowspan="1" colspan="1">Age</td>
<td align="left" rowspan="1" colspan="1">Extraversion</td>
<td align="left" rowspan="1" colspan="1">Agreeableness</td>
<td align="left" rowspan="1" colspan="1">Conscientious.</td>
<td align="left" rowspan="1" colspan="1">Neuroticism</td>
<td align="left" rowspan="1" colspan="1">Openness</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">features</td>
<td align="left" rowspan="1" colspan="1"><italic>accuracy</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>R</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>R</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>R</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>R</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>R</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>R</italic></td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>LIWC</italic></td>
<td align="left" rowspan="1" colspan="1">78.4%</td>
<td align="left" rowspan="1" colspan="1">.65</td>
<td align="left" rowspan="1" colspan="1">.27</td>
<td align="left" rowspan="1" colspan="1">.25</td>
<td align="left" rowspan="1" colspan="1">.29</td>
<td align="left" rowspan="1" colspan="1">.21</td>
<td align="left" rowspan="1" colspan="1">.29</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>Topics</italic></td>
<td align="left" rowspan="1" colspan="1"><bold>87.5%</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>.80</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>.32</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>.29</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>.33</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>.28</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>.38</bold></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>WordPhrases</italic></td>
<td align="left" rowspan="1" colspan="1"><bold>91.4%</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>.83</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>.37</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>.29</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>.34</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>.29</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>.41</bold></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>WordPhrases + Topics</italic></td>
<td align="left" rowspan="1" colspan="1"><bold>91.9%</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>.84</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>.38</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>.31</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>.35</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>.31</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>.42</bold></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>Topics + LIWC</italic></td>
<td align="left" rowspan="1" colspan="1"><bold>89.2%</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>.80</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>.33</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>.29</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>.33</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>.28</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>.38</bold></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>WordPhrases + LIWC</italic></td>
<td align="left" rowspan="1" colspan="1"><bold>91.6%</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>.83</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>.38</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>.30</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>.34</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>.30</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>.41</bold></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>WordPhrases + Topics + LIWC</italic></td>
<td align="left" rowspan="1" colspan="1"><bold>91.9%</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>.84</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>.38</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>.31</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>.35</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>.31</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>.42</bold></td>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt102"><label /><p><italic>accuracy</italic>: percent predicted correctly (for discrete binary outcomes). <italic>R</italic>: Square-root of the coefficient of determination (for sequential/continuous outcomes). <italic>LIWC</italic>: <italic>A priori</italic> word-categories from Linguistic Inquiry and Word Count. <italic>Topics</italic>: Automatically created <italic>LDA</italic> topic clusters. <italic>WordPhrases</italic>: words and phrases (n-grams of size 1 to 3 passing a collocation filter). Bold indicates significant (p&lt;.01) improvement over the baseline set of features (use of <italic>LIWC</italic> alone).</p></fn></table-wrap-foot></table-wrap>
<p>For personality we saw the largest relative improvement between <italic>open-vocabulary</italic> approaches and <italic>LIWC</italic>. Our best personality <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e064" xlink:type="simple" /></inline-formula> score of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e065" xlink:type="simple" /></inline-formula> fell just above the standard “correlational upper-limit” for behavior to predict personality (a Pearson correlation of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e066" xlink:type="simple" /></inline-formula> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e067" xlink:type="simple" /></inline-formula>) <xref ref-type="bibr" rid="pone.0073791-Meyer1">[94]</xref>, <xref ref-type="bibr" rid="pone.0073791-Roberts1">[95]</xref>. Some researchers have discretized the personality scores for prediction, and classified people as being high or low (one standard deviation above or below the mean or top and bottom quartiles, throwing out the middle) in each trait <xref ref-type="bibr" rid="pone.0073791-Argamon4">[61]</xref>, <xref ref-type="bibr" rid="pone.0073791-Mairesse2">[64]</xref>, <xref ref-type="bibr" rid="pone.0073791-Iacobelli1">[67]</xref>. When we do such an approach, our scores are in similar ranges to such literature: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e068" xlink:type="simple" /></inline-formula> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e069" xlink:type="simple" /></inline-formula> classification accuracy. Of course, such a high/low model cannot directly be used for classifying unlabeled people as one would also need to know who fits in the middle. Regression is a more appropriate predictive task for continuous outcomes like age and personality, even though <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e070" xlink:type="simple" /></inline-formula> scores are naturally smaller than binary classification accuracies.</p>
<p>We ran an additional tests to evaluate only those words and phrases, topics, or <italic>LIWC</italic> categories that are selected via differential language analysis rather than all features. Thus, we used only those language features that significantly correlated (Bonferonni-corrected <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e071" xlink:type="simple" /></inline-formula>) with the outcome being predicting. To keep consistent with the main evaluation, we used no controls, and so one could view this as a univariate feature selection over each type of feature independently. We again found significant improvement from using the open-vocabulary features over <italic>LIWC</italic> and no significant changes in accuracy overall. These results are presented in <xref ref-type="supplementary-material" rid="pone.0073791.s004">Table S2</xref>.</p>
<p>In addition to demonstrating the greater informative value of <italic>open-vocabulary</italic> features, we found our results to be state-of-the-art. The highest previous <italic>out-of-sample</italic> accuracies for gender prediction based <italic>entirely</italic> on language were 88.0% over twitter data <xref ref-type="bibr" rid="pone.0073791-Bamman1">[68]</xref> while our classifiers reach an accuracy of <italic>91.9%</italic>. Our increased performance could be attributed to our set of language features, a strong predictive algorithm (the support vector machine), and the large sample of Facebook data.</p>
</sec></sec><sec id="s5">
<title>Discussion</title>
<p>Online social media such as Facebook are a particularly promising resource for the study of people, as “status” updates are self-descriptive, personal, and have emotional content <xref ref-type="bibr" rid="pone.0073791-Kramer1">[7]</xref>. Language use is objective and quantifiable behavioral data <xref ref-type="bibr" rid="pone.0073791-Ireland1">[96]</xref>, and unlike surveys and questionnaires, Facebook language allows researchers to observe individuals as they freely present themselves in their own words. <italic>Differential language analysis</italic> (<italic>DLA</italic>) in social media is an unobtrusive and non-reactive window into the social and psychological characteristics of people's everyday concerns.</p>
<p>Most studies linking language with psychological variables rely on <italic>a priori</italic> fixed sets of words, such as the <italic>LIWC</italic> categories carefully constructed over 20 years of human research <xref ref-type="bibr" rid="pone.0073791-Pennebaker1">[11]</xref>. Here, we show the benefits of an <italic>open-vocabulary</italic> approach in which the words analyzed are based on the data itself. We extracted <italic>words</italic>, <italic>phrases</italic>, and <italic>topics</italic> (automatically clustered sets of words) from millions of Facebook messages and found the language that correlates most with gender, age, and five factors of personality. We discovered insights not found previously and achieved higher accuracies than <italic>LIWC</italic> when using our <italic>open-vocabulary</italic> features in a predictive model, achieving state-of-the-art accuracy in the case of gender prediction.</p>
<p>Exploratory analyses like <italic>DLA</italic> change the process from that of testing theories with observations to that of data-driven identification of new connections <xref ref-type="bibr" rid="pone.0073791-Haig1">[97]</xref>,<xref ref-type="bibr" rid="pone.0073791-Fast1">[98]</xref>. Our intention here is not a complete replacement for <italic>closed-vocabulary</italic> analyses like <italic>LIWC</italic>. When one has a specific theory in mind or a small sample size, an <italic>a priori</italic> list of words can be ideal; in an open-vocabulary approach, the concept one cares about can be drowned out by more predictive concepts. Further, it may be easier to compare static <italic>a priori</italic> categories of words across studies. However, automatically clustering words into coherent topics allows one to potentially discover categories that might not have been anticipated (e.g. sports teams, kinds of outdoor exercise, or Japanese cartoons). Open-vocabulary approaches also save labor in creating categories. They consider all words encountered and thus are able to adapt well to the evolving language in social media or other genres. They are also transparent in that the exact words driving correlations are not hidden behind a level of abstraction. Given lots of text and dependent variables, an open-vocabulary approach like <italic>DLA</italic> can be immediately useful for many areas of study; for example, an economist contrasting sport utility with hybrid vehicle drivers, a political scientist comparing democrats and republicans, or a cardiologist differentiating people with positive versus negative outcomes of heart disease.</p>
<p>Like most studies in the social sciences, this work is still subject to sampling and social desirability biases. Language connections with psychosocial variables are often dependent on context <xref ref-type="bibr" rid="pone.0073791-Eckert1">[40]</xref>. Here, we examined language in a large sample of the broad context of Facebook. Under different contexts, it is likely some results would differ. Still, the sample sizes and availability of demographic information afforded by social media bring us closer to a more ideal representative sample <xref ref-type="bibr" rid="pone.0073791-Gosling2">[99]</xref>. Our current results have face validity (subjects in high elevations talk about ‘the mountains’), tie in with other research (neurotic people disproportionately use the phrase ‘depressed’), suggest new hypotheses (an active life implies emotional stability), and give detailed insights (males prefer to precede ‘wife’ with the possessive ‘my’ more so than females precede ‘husband’ with ‘my’).</p>
<p>Over the past one-hundred years, surveys and questionnaires have illuminated our understanding of people. We suggest that new multipurpose instruments such as <italic>DLA</italic> emerging from the field of computational social science shed new light on psychosocial phenomena.</p>
</sec><sec id="s6">
<title>Supporting Information</title>
<supplementary-material id="pone.0073791.s001" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pone.0073791.s001" position="float" xlink:type="simple" orientation="portrait"><label>Figure S1</label><caption>
<p><bold>Power analyses for all outcomes examined in this work.</bold> Number of features passing a Benjamini-Hochberg false-discovery rate of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e072" xlink:type="simple" /></inline-formula> as a function of the number of users sampled, out of the maximum 24,530 words and phrases used by at least 1% of users.</p>
<p>(TIF)</p>
</caption></supplementary-material><supplementary-material id="pone.0073791.s002" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pone.0073791.s002" position="float" xlink:type="simple" orientation="portrait"><label>Figure S2</label><caption>
<p><bold>Words, phrases, and topics most distinguishing </bold><bold><italic>agreeableness</italic></bold><bold>, </bold><bold><italic>conscientiousness</italic></bold><bold>, and </bold><bold><italic>openness</italic></bold><bold>.</bold> A. Language of high agreeableness (left) and low agreeableness (right); <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e073" xlink:type="simple" /></inline-formula>. B. Language of high conscientiousness (left) and low conscientiousness (right); <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e074" xlink:type="simple" /></inline-formula>. C. Language of openness (left) and closed to experience (right); <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e075" xlink:type="simple" /></inline-formula> (adjusted for gender and age, Bonferroni-corrected <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0073791.e076" xlink:type="simple" /></inline-formula>).</p>
<p>(TIF)</p>
</caption></supplementary-material><supplementary-material id="pone.0073791.s003" mimetype="application/vnd.ms-excel" xlink:href="info:doi/10.1371/journal.pone.0073791.s003" position="float" xlink:type="simple" orientation="portrait"><label>Table S1</label><caption>
<p><bold>The 15 most prevalent words for the 2000 automatically generated </bold><bold><italic>topics</italic></bold><bold> used in our study.</bold> All topics available here: wwbp.org/public_data/2000topics.top20freqs.keys.csv.</p>
<p>(XLS)</p>
</caption></supplementary-material><supplementary-material id="pone.0073791.s004" mimetype="application/pdf" xlink:href="info:doi/10.1371/journal.pone.0073791.s004" position="float" xlink:type="simple" orientation="portrait"><label>Table S2</label><caption>
<p><bold>Prediction results when selecting features via differential language analysis.</bold> <italic>accuracy</italic>: percent predicted correctly (for discrete binary outcomes). <italic>R</italic>: Square-root of the coefficient of determination (for sequential/continuous outcomes). <italic>LIWC</italic>: <italic>A priori</italic> word-categories from Linguistic Inquiry and Word Count. <italic>Topics</italic>: Automatically created <italic>LDA</italic> topic clusters. <italic>WordPhrases</italic>: words and phrases (n-grams of size 1 to 3 passing a collocation filter). Bold indicates significant (<italic>P</italic>&lt;.01) improvement over the baseline set of features (use of <italic>LIWC</italic> alone). Differential language analysis was run over the training set, and only those features significant at Bonferonni-corrected <italic>P</italic>&lt;0.001 were included during training and testing. No controls were used so as to be consistent with the evaluation in the main paper, and so one could consider this a univariate feature selection. On average results are just below those of not using <italic>differential language analysis</italic> to select features but there is no significant difference.</p>
<p>(PDF)</p>
</caption></supplementary-material></sec></body>
<back>
<ack>
<p>We would like to thank Greg Park, Angela Duckworth, Adam Croom, Molly Ireland, Paul Rozin, Eduardo Blanco, and our other colleagues in the Positive Psychology Center and Computer &amp; Information Science department for their valuable feedback regarding this work.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pone.0073791-Lazer1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lazer</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Pentland</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Adamic</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Aral</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Barabasi</surname><given-names>AL</given-names></name>, <etal>et al</etal>. (<year>2009</year>) <article-title>Computational social science</article-title>. <source>Science</source> <volume>323</volume>: <fpage>721</fpage>–<lpage>723</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Weinberger1"><label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Weinberger</surname><given-names>S</given-names></name> (<year>2011</year>) <article-title>Web of war: Can computational social science help to prevent or win wars? the pentagon is betting millions of dollars on the hope that it will</article-title>. <source>Nature</source> <volume>471</volume>: <fpage>566</fpage>–<lpage>568</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Miller1"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Miller</surname><given-names>G</given-names></name> (<year>2011</year>) <article-title>Social scientists wade into the tweet stream</article-title>. <source>Science</source> <volume>333</volume>: <fpage>1814</fpage>–<lpage>1815</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Facebook1"><label>4</label>
<mixed-citation publication-type="other" xlink:type="simple">Facebook (2012) Facebook company info: Fact sheet website. Available: <ext-link ext-link-type="uri" xlink:href="http://newsroom" xlink:type="simple">http://newsroom</ext-link>⋅fb⋅com. Accessed 2012 Dec.</mixed-citation>
</ref>
<ref id="pone.0073791-Golder1"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Golder</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Macy</surname><given-names>M</given-names></name> (<year>2011</year>) <article-title>Diurnal and seasonal mood vary with work, sleep, and daylength across diverse cultures</article-title>. <source>Science</source> <volume>333</volume>: <fpage>1878</fpage>–<lpage>1881</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Bollen1"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bollen</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Mao</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Zeng</surname><given-names>X</given-names></name> (<year>2011</year>) <article-title>Twitter mood predicts the stock market</article-title>. <source>Journal of Computational Science</source> <volume>2</volume>: <fpage>1</fpage>–<lpage>8</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Kramer1"><label>7</label>
<mixed-citation publication-type="other" xlink:type="simple">Kramer A (2010) An unobtrusive behavioral model of gross national happiness. In: Proc of the 28th int conf on Human factors in comp sys. ACM, pp. 287–290.</mixed-citation>
</ref>
<ref id="pone.0073791-Dodds1"><label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dodds</surname><given-names>PS</given-names></name>, <name name-style="western"><surname>Harris</surname><given-names>KD</given-names></name>, <name name-style="western"><surname>Kloumann</surname><given-names>IM</given-names></name>, <name name-style="western"><surname>Bliss</surname><given-names>CA</given-names></name>, <name name-style="western"><surname>Danforth</surname><given-names>CM</given-names></name> (<year>2011</year>) <article-title>Temporal patterns of happiness and information in a global social network: Hedonometrics and twitter</article-title>. <source>PLoS ONE</source> <volume>6</volume>: <fpage>26</fpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Ginsberg1"><label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ginsberg</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Mohebbi</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Patel</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Brammer</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Smolinski</surname><given-names>M</given-names></name>, <etal>et al</etal>. (<year>2009</year>) <article-title>Detecting inuenza epidemics using search engine query data</article-title>. <source>Nature</source> <volume>457</volume>: <fpage>1012</fpage>–<lpage>1014</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Michel1"><label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Michel</surname><given-names>JB</given-names></name>, <name name-style="western"><surname>Shen</surname><given-names>YK</given-names></name>, <name name-style="western"><surname>Aiden</surname><given-names>AP</given-names></name>, <name name-style="western"><surname>Veres</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Gray</surname><given-names>MK</given-names></name>, <etal>et al</etal>. (<year>2011</year>) <article-title>Quantitative analysis of culture using millions of digitized books</article-title>. <source>Science</source> <volume>331</volume>: <fpage>176</fpage>–<lpage>182</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Pennebaker1"><label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pennebaker</surname><given-names>JW</given-names></name>, <name name-style="western"><surname>Chung</surname><given-names>CK</given-names></name>, <name name-style="western"><surname>Ireland</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Gonzales</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Booth</surname><given-names>RJ</given-names></name> (<year>2007</year>) <article-title>The development and psychometric properties of liwc2007 the university of texas at austin</article-title>. <source>LIWCNET</source> <volume>1</volume>: <fpage>1</fpage>–<lpage>22</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Kosinski1"><label>12</label>
<mixed-citation publication-type="other" xlink:type="simple">Kosinski M, Stillwell D, Graepel Y (2013) Private traits and attributes are predictable from digital records of human behavior. Proceedings of the National Academy of Sciences (PNAS).</mixed-citation>
</ref>
<ref id="pone.0073791-Goldberg1"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Goldberg</surname><given-names>LR</given-names></name> (<year>1990</year>) <article-title>An alternative “description of personality”: the big-five factor structure</article-title>. <source>J Pers and Soc Psychol</source> <volume>59</volume>: <fpage>1216</fpage>–<lpage>1229</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-McCrae1"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McCrae</surname><given-names>RR</given-names></name>, <name name-style="western"><surname>John</surname><given-names>OP</given-names></name> (<year>1992</year>) <article-title>An introduction to the five-factor model and its applications</article-title>. <source>Journal of Personality</source> <volume>60</volume>: <fpage>175</fpage>–<lpage>215</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Norman1"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Norman</surname><given-names>W</given-names></name> (<year>1963</year>) <article-title>Toward an adequate taxonomy of personality attributes: Replicated factor structure in peer nomination personality ratings</article-title>. <source>The Journal of Abnormal and Social Psychology</source> <volume>66</volume>: <fpage>574</fpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Digman1"><label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Digman</surname><given-names>J</given-names></name> (<year>1990</year>) <article-title>Personality structure: Emergence of the five-factor model</article-title>. <source>Annual review of psychology</source> <volume>41</volume>: <fpage>417</fpage>–<lpage>440</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Stone1"><label>17</label>
<mixed-citation publication-type="other" xlink:type="simple">Stone P, Dunphy D, Smith M (1966) The General Inquirer: A Computer Approach to Content Analysis. MIT press.</mixed-citation>
</ref>
<ref id="pone.0073791-Coltheart1"><label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Coltheart</surname><given-names>M</given-names></name> (<year>1981</year>) <article-title>The mrc psycholinguistic database</article-title>. <source>The Quarterly Journal of Experimental Psychology</source> <volume>33</volume>: <fpage>497</fpage>–<lpage>505</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Pennebaker2"><label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pennebaker</surname><given-names>JW</given-names></name>, <name name-style="western"><surname>Mehl</surname><given-names>MR</given-names></name>, <name name-style="western"><surname>Niederhoffer</surname><given-names>KG</given-names></name> (<year>2003</year>) <article-title>Psychological aspects of natural language use: our words, our selves</article-title>. <source>Annual Review of Psychology</source> <volume>54</volume>: <fpage>547</fpage>–<lpage>77</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Tausczik1"><label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tausczik</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Pennebaker</surname><given-names>J</given-names></name> (<year>2010</year>) <article-title>The psychological meaning of words: Liwc and computerized text analysis methods</article-title>. <source>Journal of Language and Social Psychology</source> <volume>29</volume>: <fpage>24</fpage>–<lpage>54</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Pennebaker3"><label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pennebaker</surname><given-names>J</given-names></name>, <name name-style="western"><surname>King</surname><given-names>L</given-names></name> (<year>1999</year>) <article-title>Linguistic styles: language use as an individual difference</article-title>. <source>Journal of personality and social psychology</source> <volume>77</volume>: <fpage>1296</fpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Mehl1"><label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mehl</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Gosling</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Pennebaker</surname><given-names>J</given-names></name> (<year>2006</year>) <article-title>Personality in its natural habitat: manifestations and implicit folk theories of personality in daily life</article-title>. <source>Journal of personality and social psychology</source> <volume>90</volume>: <fpage>862</fpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Gosling1"><label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gosling</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Vazire</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Srivastava</surname><given-names>S</given-names></name>, <name name-style="western"><surname>John</surname><given-names>O</given-names></name> (<year>2004</year>) <article-title>Should we trust web-based studies? a comparative analysis of six preconceptions about internet questionnaires</article-title>. <source>American Psychologist</source> <volume>59</volume>: <fpage>93</fpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Back1"><label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Back</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Stopfer</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Vazire</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Gaddis</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Schmukle</surname><given-names>S</given-names></name>, <etal>et al</etal>. (<year>2010</year>) <article-title>Facebook profiles reect actual personality, not self-idealization</article-title>. <source>Psychological Science</source> <volume>21</volume>: <fpage>372</fpage>–<lpage>374</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Sumner1"><label>25</label>
<mixed-citation publication-type="other" xlink:type="simple">Sumner C, Byers A, Shearing M (2011) Determining personality traits &amp; privacy concerns from facebook activity. In: Black Hat Briefings. 1–29.</mixed-citation>
</ref>
<ref id="pone.0073791-Holtgraves1"><label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Holtgraves</surname><given-names>T</given-names></name> (<year>2011</year>) <article-title>Text messaging, personality, and the social context</article-title>. <source>Journal of Research in Personality</source> <volume>45</volume>: <fpage>92</fpage>–<lpage>99</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Yarkoni1"><label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yarkoni</surname><given-names>T</given-names></name> (<year>2010</year>) <article-title>Personality in 100,000 Words: A large-scale analysis of personality and word use among bloggers</article-title>. <source>Journal of Research in Personality</source> <volume>44</volume>: <fpage>363</fpage>–<lpage>373</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Chung1"><label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chung</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Pennebaker</surname><given-names>J</given-names></name> (<year>2008</year>) <article-title>Revealing dimensions of thinking in open-ended self-descriptions: An automated meaning extraction method for natural language</article-title>. <source>Journal of Research in Personality</source> <volume>42</volume>: <fpage>96</fpage>–<lpage>132</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Kramer2"><label>29</label>
<mixed-citation publication-type="other" xlink:type="simple">Kramer A, Chung K (2011) Dimensions of self-expression in facebook status updates. In: Proceedings of the Fifth International AAAI Conference on Weblogs and Social Media. 169–176.</mixed-citation>
</ref>
<ref id="pone.0073791-Pennebaker4"><label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pennebaker</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Stone</surname><given-names>L</given-names></name> (<year>2003</year>) <article-title>Words of wisdom: Language use over the life span</article-title>. <source>Journal of personality and social psychology</source> <volume>85</volume>: <fpage>291</fpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Chung2"><label>31</label>
<mixed-citation publication-type="other" xlink:type="simple">Chung C, Pennebaker J (2007) The psychological function of function words. Social communication: Frontiers of social psychology : 343–359.</mixed-citation>
</ref>
<ref id="pone.0073791-Argamon1"><label>32</label>
<mixed-citation publication-type="other" xlink:type="simple">Argamon S, Koppel M, Pennebaker J, Schler J (2007) Mining the blogosphere: age, gender, and the varieties of self-expression. First Monday 12.</mixed-citation>
</ref>
<ref id="pone.0073791-Argamon2"><label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Argamon</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Koppel</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Fine</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Shimoni</surname><given-names>A</given-names></name> (<year>2003</year>) <article-title>Gender, genre, and writing style in formal written texts</article-title>. <source>To appear in Text</source> <volume>23</volume>: <fpage>3</fpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Newman1"><label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Newman</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Groom</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Handelman</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Pennebaker</surname><given-names>J</given-names></name> (<year>2008</year>) <article-title>Gender differences in language use: An analysis of 14,000 text samples</article-title>. <source>Discourse Processes</source> <volume>45</volume>: <fpage>211</fpage>–<lpage>236</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Mukherjee1"><label>35</label>
<mixed-citation publication-type="other" xlink:type="simple">Mukherjee A, Liu B (2010) Improving gender classification of blog authors. In: Proceedings of the 2010 conference on Empirical Methods in natural Language Processing. Association for Computational Linguistics, pp. 207–217.</mixed-citation>
</ref>
<ref id="pone.0073791-Rao1"><label>36</label>
<mixed-citation publication-type="other" xlink:type="simple">Rao D, Yarowsky D, Shreevats A, Gupta M (2010) Classifying latent user attributes in twitter. In: Proceedings of the 2nd international workshop on Search and mining user-generated contents. ACM, pp. 37–44.</mixed-citation>
</ref>
<ref id="pone.0073791-Schler1"><label>37</label>
<mixed-citation publication-type="other" xlink:type="simple">Schler J, Koppel M, Argamon S, Pennebaker J (2006) Effects of age and gender on blogging. In: Proceedings of 2006 AAAI Spring Symposium on Computational Approaches for Analyzing Weblogs. pp. 199–205.</mixed-citation>
</ref>
<ref id="pone.0073791-Burger1"><label>38</label>
<mixed-citation publication-type="other" xlink:type="simple">Burger J, Henderson J, Kim G, Zarrella G (2011) Discriminating gender on twitter. In: Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, pp. 1301–1309.</mixed-citation>
</ref>
<ref id="pone.0073791-Huffaker1"><label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Huffaker</surname><given-names>DA</given-names></name>, <name name-style="western"><surname>Calvert</surname><given-names>SL</given-names></name> (<year>2005</year>) <article-title>Gender, Identity, and Language Use in Teenage Blogs</article-title>. <source>Journal of Computer-Mediated Communication</source> <volume>10</volume>: <fpage>1</fpage>–<lpage>10</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Eckert1"><label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Eckert</surname><given-names>P</given-names></name> (<year>2008</year>) <article-title>Variation and the indexical field1</article-title>. <source>Journal of Sociolinguistics</source> <volume>12</volume>: <fpage>453</fpage>–<lpage>476</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Eisenstein1"><label>41</label>
<mixed-citation publication-type="other" xlink:type="simple">Eisenstein J, Smith NA, Xing EP (2011) Discovering sociolinguistic associations with structured sparsity. In: Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1. Association for Computational Linguistics, pp. 1365–1374.</mixed-citation>
</ref>
<ref id="pone.0073791-OConnor1"><label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>OConnor</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Bamman</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Smith</surname><given-names>NA</given-names></name> (<year>2011</year>) <article-title>Computational text analysis for social science: Model assumptions and complexity</article-title>. <source>public health</source> <volume>41</volume>: <fpage>43</fpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Grimmer1"><label>43</label>
<mixed-citation publication-type="other" xlink:type="simple">Grimmer J, Stewart BM (2013) Text as data: The promise and pitfalls of automatic content analysis methods for political texts. Political Analysis.</mixed-citation>
</ref>
<ref id="pone.0073791-Monroe1"><label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Monroe</surname><given-names>BL</given-names></name>, <name name-style="western"><surname>Colaresi</surname><given-names>MP</given-names></name>, <name name-style="western"><surname>Quinn</surname><given-names>KM</given-names></name> (<year>2008</year>) <article-title>Fightin'words: Lexical feature selection and evaluation for identifying the content of political conict</article-title>. <source>Political Analysis</source> <volume>16</volume>: <fpage>372</fpage>–<lpage>403</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Gilbert1"><label>45</label>
<mixed-citation publication-type="other" xlink:type="simple">Gilbert E (2012) Phrases that signal workplace hierarchy. In: Proceedings of the ACM 2012 conference on Computer Supported Cooperative Work. ACM, pp. 1037–1046.</mixed-citation>
</ref>
<ref id="pone.0073791-Tausczik2"><label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tausczik</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Pennebaker</surname><given-names>J</given-names></name> (<year>2010</year>) <article-title>The psychological meaning of words: Liwc and computerized text analysis methods</article-title>. <source>Journal of Language and Social Psychology</source> <volume>29</volume>: <fpage>24</fpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Holmes1"><label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Holmes</surname><given-names>D</given-names></name> (<year>1994</year>) <article-title>Authorship attribution</article-title>. <source>Computers and the Humanities</source> <volume>28</volume>: <fpage>87</fpage>–<lpage>106</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Argamon3"><label>48</label>
<mixed-citation publication-type="other" xlink:type="simple">Argamon S, Šarić M, Stein SS (2003) Style mining of electronic messages for multiple authorship discrimination: first results. In: KDD '03: Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining. New York, NY, USA: ACM, pp. 475–480.</mixed-citation>
</ref>
<ref id="pone.0073791-Stamatatos1"><label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stamatatos</surname><given-names>E</given-names></name> (<year>2009</year>) <article-title>A survey of modern authorship attribution methods</article-title>. <source>Journal of the American Society for information Science and Technology</source> <volume>60</volume>: <fpage>538</fpage>–<lpage>556</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Alm1"><label>50</label>
<mixed-citation publication-type="other" xlink:type="simple">Alm C, Roth D, Sproat R (2005) Emotions from text: machine learning for text-based emotion prediction. In: Proceedings of the conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, pp. 579–586.</mixed-citation>
</ref>
<ref id="pone.0073791-Mihalcea1"><label>51</label>
<mixed-citation publication-type="other" xlink:type="simple">Mihalcea R, Liu H (2006) A corpus-based approach to finding happiness. In: Proceedings of the AAAI Spring Symposium on Computational Approaches to Weblogs. p. 19.</mixed-citation>
</ref>
<ref id="pone.0073791-Jurafsky1"><label>52</label>
<mixed-citation publication-type="other" xlink:type="simple">Jurafsky D, Ranganath R, McFarland D (2009) Extracting social meaning: Identifying interactional style in spoken conversation. In: Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, pp. 638–646.</mixed-citation>
</ref>
<ref id="pone.0073791-Ranganath1"><label>53</label>
<mixed-citation publication-type="other" xlink:type="simple">Ranganath R, Jurafsky D, McFarland D (2009) It's not you, it's me: detecting irting and its misperception in speed-dates. In: Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1. Association for Computational Linguistics, pp. 334–342.</mixed-citation>
</ref>
<ref id="pone.0073791-Pang1"><label>54</label>
<mixed-citation publication-type="other" xlink:type="simple">Pang B, Lee L, Vaithyanathan S (2002) Thumbs up? Sentiment classification using machine learning techniques. In: Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP). 79–86.</mixed-citation>
</ref>
<ref id="pone.0073791-Kim1"><label>55</label>
<mixed-citation publication-type="other" xlink:type="simple">Kim SM, Hovy E (2004) Determining the sentiment of opinions. In: Proceedings of the 20th international conference on Computational Linguistics. Stroudsburg, PA, USA: Association for Computational Linguistics, COLING,04.</mixed-citation>
</ref>
<ref id="pone.0073791-Wilson1"><label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wilson</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Wiebe</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Hoffmann</surname><given-names>P</given-names></name> (<year>2009</year>) <article-title>Recognizing contextual polarity: An exploration of features for phrase-level sentiment analysis</article-title>. <source>Computational linguistics</source> <volume>35</volume>: <fpage>399</fpage>–<lpage>433</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Baccianella1"><label>57</label>
<mixed-citation publication-type="other" xlink:type="simple">Baccianella S, Esuli A, Sebastiani F (2010) Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In: Chair) NCC, Choukri K, Maegaard B, Mariani J, Odijk J, et al., editors, Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC'10). Valletta, Malta: European Language Resources Association (ELRA).</mixed-citation>
</ref>
<ref id="pone.0073791-Laver1"><label>58</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Laver</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Benoit</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Garry</surname><given-names>J</given-names></name> (<year>2003</year>) <article-title>Extracting policy positions from political texts using words as data</article-title>. <source>American Political Science Review</source> <volume>97</volume>: <fpage>311</fpage>–<lpage>331</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Monroe2"><label>59</label>
<mixed-citation publication-type="other" xlink:type="simple">Monroe BL, Maeda K (2004) Talks cheap: Text-based estimation of rhetorical ideal-points. In: annual meeting of the Society for Political Methodology. 29–31.</mixed-citation>
</ref>
<ref id="pone.0073791-Slapin1"><label>60</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Slapin</surname><given-names>JB</given-names></name>, <name name-style="western"><surname>Proksch</surname><given-names>SO</given-names></name> (<year>2008</year>) <article-title>A scaling model for estimating time-series party positions from texts</article-title>. <source>American Journal of Political Science</source> <volume>52</volume>: <fpage>705</fpage>–<lpage>722</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Argamon4"><label>61</label>
<mixed-citation publication-type="other" xlink:type="simple">Argamon S, Dhawle S, Koppel M, Pennebaker JW (2005) Lexical predictors of personality type. In: Proceedings of the Joint Annual Meeting of the Interface and the Classification Society.</mixed-citation>
</ref>
<ref id="pone.0073791-Argamon5"><label>62</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Argamon</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Koppel</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Pennebaker</surname><given-names>JW</given-names></name>, <name name-style="western"><surname>Schler</surname><given-names>J</given-names></name> (<year>2009</year>) <article-title>Automatically profiling the author of an anonymous text</article-title>. <source>Commun ACM</source> <volume>52</volume>: <fpage>119</fpage>–<lpage>123</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Mairesse1"><label>63</label>
<mixed-citation publication-type="other" xlink:type="simple">Mairesse F,Walker M (2006) Automatic recognition of personality in conversation. In: Proceedings of the Human Language Technology Conference of the NAACL. 85–88.</mixed-citation>
</ref>
<ref id="pone.0073791-Mairesse2"><label>64</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mairesse</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Walker</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Mehl</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Moore</surname><given-names>R</given-names></name> (<year>2007</year>) <article-title>Using linguistic cues for the automatic recognition of personality in conversation and text</article-title>. <source>Journal of Artificial Intelligence Research</source> <volume>30</volume>: <fpage>457</fpage>–<lpage>500</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Golbeck1"><label>65</label>
<mixed-citation publication-type="other" xlink:type="simple">Golbeck J, Robles C, Edmondson M, Turner K (2011) Predicting personality from twitter. In: Proc of the 3rd IEEE Int Conf on Soc Comput. 149–156. doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/978-0-7695-4578-3/11" xlink:type="simple">978-0-7695-4578-3/11</ext-link>.</mixed-citation>
</ref>
<ref id="pone.0073791-Sumner2"><label>66</label>
<mixed-citation publication-type="other" xlink:type="simple">Sumner C, Byers A, Boochever R, Park G (2012) Predicting dark triad personality traits from twitter usage and a linguistic analysis of tweets. wwwonlineprivacyfoundationorg.</mixed-citation>
</ref>
<ref id="pone.0073791-Iacobelli1"><label>67</label>
<mixed-citation publication-type="other" xlink:type="simple">Iacobelli F, Gill AJ, Nowson S, Oberlander J (2011) Large scale personality classification of bloggers. In: Proc of the 4th int conf on Affect comput and intel interaction. Springer-Verlag, pp. 568–577.</mixed-citation>
</ref>
<ref id="pone.0073791-Bamman1"><label>68</label>
<mixed-citation publication-type="other" xlink:type="simple">Bamman D, Eisenstein J, Schnoebelen T (2012) Gender in twitter: Styles, stances, and social networks. arXiv preprint arXiv:12104567.</mixed-citation>
</ref>
<ref id="pone.0073791-Church1"><label>69</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Church</surname><given-names>KW</given-names></name>, <name name-style="western"><surname>Hanks</surname><given-names>P</given-names></name> (<year>1990</year>) <article-title>Word association norms, mutual information, and lexicography</article-title>. <source>Computational Linguistics</source> <volume>16</volume>: <fpage>22</fpage>–<lpage>29</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Lin1"><label>70</label>
<mixed-citation publication-type="other" xlink:type="simple">Lin D (1998) Extracting collocations from text corpora. In: Knowledge Creation Diffusion Utilization. 57–63.</mixed-citation>
</ref>
<ref id="pone.0073791-Anscombe1"><label>71</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Anscombe</surname><given-names>FJ</given-names></name> (<year>1948</year>) <article-title>The transformation of poisson, binomial and negative-binomial data</article-title>. <source>Biometrika</source> <volume>35</volume>: <fpage>246</fpage>–<lpage>254</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Blei1"><label>72</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Blei</surname><given-names>DM</given-names></name>, <name name-style="western"><surname>Ng</surname><given-names>AY</given-names></name>, <name name-style="western"><surname>Jordan</surname><given-names>MI</given-names></name> (<year>2003</year>) <article-title>Latent dirichlet allocation</article-title>. <source>J Mach Learn Res</source> <volume>3</volume>: <fpage>993</fpage>–<lpage>1022</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Steyvers1"><label>73</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Steyvers</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Griffiths</surname><given-names>T</given-names></name> (<year>2007</year>) <article-title>Probabilistic topic models</article-title>. <source>Handbook of latent semantic analysis</source> <volume>427</volume>: <fpage>424</fpage>–<lpage>440</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Gelfand1"><label>74</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gelfand</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Smith</surname><given-names>A</given-names></name> (<year>1990</year>) <article-title>Sampling-based approaches to calculating marginal densities</article-title>. <source>Journal of the American statistical association</source> <volume>85</volume>: <fpage>398</fpage>–<lpage>409</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-McCallum1"><label>75</label>
<mixed-citation publication-type="other" xlink:type="simple">McCallum AK (2002) Mallet: A machine learning for language toolkit. Available: <ext-link ext-link-type="uri" xlink:href="http://mallet.cs.umass.edu" xlink:type="simple">http://mallet.cs.umass.edu</ext-link>.</mixed-citation>
</ref>
<ref id="pone.0073791-Dunn1"><label>76</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dunn</surname><given-names>OJ</given-names></name> (<year>1961</year>) <article-title>Multiple comparisons among means</article-title>. <source>Journal of the American Statistical Association</source> <volume>56</volume>: <fpage>52</fpage>–<lpage>64</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Eisenstein2"><label>77</label>
<mixed-citation publication-type="other" xlink:type="simple">Eisenstein J, O'Connor B, Smith N, Xing E (2010) A latent variable model for geographic lexical variation. In: Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, pp. 1277–1287.</mixed-citation>
</ref>
<ref id="pone.0073791-Wordle1"><label>78</label>
<mixed-citation publication-type="other" xlink:type="simple">Wordle (2012) Wordle advanced website. Available: <ext-link ext-link-type="uri" xlink:href="http://www" xlink:type="simple">http://www</ext-link>⋅wordle⋅net/advanced Acceessed 2012 Dec.</mixed-citation>
</ref>
<ref id="pone.0073791-Harris1"><label>79</label>
<mixed-citation publication-type="other" xlink:type="simple">Harris J (2011) Word clouds considered harmful. Available: <ext-link ext-link-type="uri" xlink:href="http://wwwniemanlaborg/2011/10/word-clouds-considered-harmful/" xlink:type="simple">http://wwwniemanlaborg/2011/10/word-clouds-considered-harmful/</ext-link>.</mixed-citation>
</ref>
<ref id="pone.0073791-Resnik1"><label>80</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Resnik</surname><given-names>P</given-names></name> (<year>1999</year>) <article-title>Semantic similarity in a taxonomy: An information-based measure and its application to problems of ambiguity in natural language</article-title>. <source>Journal of Artificial Intelligence Research</source> <volume>11</volume>: <fpage>95</fpage>–<lpage>130</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Cleveland1"><label>81</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cleveland</surname><given-names>WS</given-names></name> (<year>1979</year>) <article-title>Robust locally weighted regression and smoothing scatterplots</article-title>. <source>Journal of the Am Stati Assoc</source> <volume>74</volume>: <fpage>829</fpage>–<lpage>836</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-CostaJr1"><label>82</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Costa Jr</surname><given-names>P</given-names></name>, <name name-style="western"><surname>McCrae</surname><given-names>R</given-names></name> (<year>2008</year>) <article-title>The revised neo personality inventory (neo-pi-r)</article-title>. <source>The SAGE handbook of personality theory and assessment</source> <volume>2</volume>: <fpage>179</fpage>–<lpage>198</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Bachrach1"><label>83</label>
<mixed-citation publication-type="other" xlink:type="simple">Bachrach Y, Kosinski M, Graepel T, Kohli P, Stillwell D (2012) Personality and patterns of facebook usage. Web Science.</mixed-citation>
</ref>
<ref id="pone.0073791-Sterne1"><label>84</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sterne</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Gavaghan</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Egger</surname><given-names>M</given-names></name> (<year>2000</year>) <article-title>Publication and related bias in meta-analysis: power of statistical tests and prevalence in the literature</article-title>. <source>J Clin Epidemiol</source> <volume>53</volume>: <fpage>1119</fpage>–<lpage>1129</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-McCrae2"><label>85</label>
<mixed-citation publication-type="other" xlink:type="simple">McCrae RR, Sutin AR (2009) Openness to experience. In: Handbook of Indiv Diff in Soc Behav, New York: Guilford. 257–273.</mixed-citation>
</ref>
<ref id="pone.0073791-Mulac1"><label>86</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mulac</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Studley</surname><given-names>LB</given-names></name>, <name name-style="western"><surname>Blau</surname><given-names>S</given-names></name> (<year>1990</year>) <article-title>The gender-linked language effect in primary and secondary students' impromptu essays</article-title>. <source>Sex Roles</source> <volume>23</volume>: <fpage>439</fpage>–<lpage>470</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Thomson1"><label>87</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Thomson</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Murachver</surname><given-names>T</given-names></name> (<year>2001</year>) <article-title>Predicting gender from electronic discourse</article-title>. <source>Brit J of Soc Psychol</source> <volume>40</volume>: <fpage>193</fpage>–<lpage>208</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Mehl2"><label>88</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mehl</surname><given-names>MR</given-names></name>, <name name-style="western"><surname>Pennebaker</surname><given-names>JW</given-names></name> (<year>2003</year>) <article-title>The sounds of social life: a psychometric analysis of students' daily social environments and natural conversations</article-title>. <source>J of Pers and Soc Psychol</source> <volume>84</volume>: <fpage>857</fpage>–<lpage>870</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Mulac2"><label>89</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mulac</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Bradac</surname><given-names>JJ</given-names></name> (<year>1986</year>) <article-title>Male/female language differences and attributional consequences in a public speaking situation: Toward an explanation of the genderlinked language effect</article-title>. <source>Communication Monographs</source> <volume>53</volume>: <fpage>115</fpage>–<lpage>129</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Benjamini1"><label>90</label>
<mixed-citation publication-type="other" xlink:type="simple">Benjamini Y, Hochberg Y (1995) Controlling the false discovery rate: a practical and powerful approach to multiple testing. Journal of the Royal Statistical Society Series B (Methodological) : 289–300.</mixed-citation>
</ref>
<ref id="pone.0073791-Goldberg2"><label>91</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Goldberg</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Johnson</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Eber</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Hogan</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Ashton</surname><given-names>M</given-names></name>, <etal>et al</etal>. (<year>2006</year>) <article-title>The international personality item pool and the future of public-domain personality measures</article-title>. <source>J of Res in Personal</source> <volume>40</volume>: <fpage>84</fpage>–<lpage>96</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Fan1"><label>92</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fan</surname><given-names>RE</given-names></name>, <name name-style="western"><surname>Chang</surname><given-names>KW</given-names></name>, <name name-style="western"><surname>Hsieh</surname><given-names>CJ</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>XR</given-names></name>, <name name-style="western"><surname>Lin</surname><given-names>CJ</given-names></name> (<year>2008</year>) <article-title>LIBLINEAR: A library for large linear classification</article-title>. <source>Journal of Machine Learning Research</source> <volume>9</volume>: <fpage>1871</fpage>–<lpage>1874</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Hoerl1"><label>93</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hoerl</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Kennard</surname><given-names>R</given-names></name> (<year>1970</year>) <article-title>Ridge regression: Biased estimation for nonorthogonal problems</article-title>. <source>Technometrics</source> <volume>12</volume>: <fpage>55</fpage>–<lpage>67</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Meyer1"><label>94</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Meyer</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Finn</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Eyde</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Kay</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Moreland</surname><given-names>K</given-names></name>, <etal>et al</etal>. (<year>2001</year>) <article-title>Psychological testing and psychological assessment: A review of evidence and issues</article-title>. <source>American psychologist</source> <volume>56</volume>: <fpage>128</fpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Roberts1"><label>95</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Roberts</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Kuncel</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Shiner</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Caspi</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Goldberg</surname><given-names>L</given-names></name> (<year>2007</year>) <article-title>The power of personality: The comparative validity of personality traits, socioeconomic status, and cognitive ability for predicting important life outcomes</article-title>. <source>Perspectives on Psychological Science</source> <volume>2</volume>: <fpage>313</fpage>–<lpage>345</lpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Ireland1"><label>96</label>
<mixed-citation publication-type="other" xlink:type="simple">Ireland ME, Mehl MR (2012) Natural language use as a marker of personality. (in press) Oxford Handbook of Language and Social Psychology.</mixed-citation>
</ref>
<ref id="pone.0073791-Haig1"><label>97</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Haig</surname><given-names>B</given-names></name> (<year>2005</year>) <article-title>An abductive theory of scientific method</article-title>. <source>Psychological Methods; Psychological Methods</source> <volume>10</volume>: <fpage>371</fpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Fast1"><label>98</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fast</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Funder</surname><given-names>D</given-names></name> (<year>2008</year>) <article-title>Personality as manifest in word use: Correlations with self-report, acquaintance report, and behavior</article-title>. <source>Journal of Personality and Social Psychology</source> <volume>94</volume>: <fpage>334</fpage>.</mixed-citation>
</ref>
<ref id="pone.0073791-Gosling2"><label>99</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gosling</surname><given-names>SD</given-names></name>, <name name-style="western"><surname>Vazire</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Srivastava</surname><given-names>S</given-names></name>, <name name-style="western"><surname>John</surname><given-names>OP</given-names></name> (<year>2000</year>) <article-title>Should we trust web-based studies? a comparative analysis of six preconceptions about internet questionnaires</article-title>. <source>American Psychologist</source> <volume>59</volume>: <fpage>93</fpage>–<lpage>104</lpage>.</mixed-citation>
</ref>
</ref-list></back>
</article>