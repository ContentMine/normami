<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group>
<journal-title xml:lang="en">PLoS ONE</journal-title></journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PONE-D-13-29655</article-id>
<article-id pub-id-type="doi">10.1371/journal.pone.0080821</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories>
<title-group>
<article-title>Employing a Monte Carlo Algorithm in Newton-Type Methods for Restricted Maximum Likelihood Estimation of Genetic Parameters</article-title>
<alt-title alt-title-type="running-head">Monte Carlo Algorithm for REML</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Matilainen</surname><given-names>Kaarina</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Mäntysaari</surname><given-names>Esa A.</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Lidauer</surname><given-names>Martin H.</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Strandén</surname><given-names>Ismo</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Thompson</surname><given-names>Robin</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><label>1</label><addr-line>Biotechnology and Food Research, MTT Agrifood Research Finland, Jokioinen, Finland</addr-line></aff>
<aff id="aff2"><label>2</label><addr-line>Biomathematics and Bioinformatics Department, Rothamsted Research, Harpenden, United Kingdom</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Wu</surname><given-names>Rongling</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1" /></contrib>
</contrib-group>
<aff id="edit1"><addr-line>Pennsylvania State University, United States of America</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">kaarina.matilainen@mtt.fi</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: KM EAM MHL IS RT. Analyzed the data: KM. Wrote the paper: KM EAM MHL IS RT.</p></fn>
</author-notes>
<pub-date pub-type="collection"><year>2013</year></pub-date>
<pub-date pub-type="epub"><day>10</day><month>12</month><year>2013</year></pub-date>
<volume>8</volume>
<issue>12</issue>
<elocation-id>e80821</elocation-id>
<history>
<date date-type="received"><day>18</day><month>7</month><year>2013</year></date>
<date date-type="accepted"><day>9</day><month>10</month><year>2013</year></date>
</history>
<permissions>
<copyright-year>2013</copyright-year>
<copyright-holder>Matilainen et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>Estimation of variance components by Monte Carlo (MC) expectation maximization (EM) restricted maximum likelihood (REML) is computationally efficient for large data sets and complex linear mixed effects models. However, efficiency may be lost due to the need for a large number of iterations of the EM algorithm. To decrease the computing time we explored the use of faster converging Newton-type algorithms within MC REML implementations. The implemented algorithms were: MC Newton-Raphson (NR), where the information matrix was generated via sampling; MC average information(AI), where the information was computed as an average of observed and expected information; and MC Broyden's method, where the zero of the gradient was searched using a quasi-Newton-type algorithm. Performance of these algorithms was evaluated using simulated data. The final estimates were in good agreement with corresponding analytical ones. MC NR REML and MC AI REML enhanced convergence compared to MC EM REML and gave standard errors for the estimates as a by-product. MC NR REML required a larger number of MC samples, while each MC AI REML iteration demanded extra solving of mixed model equations by the number of parameters to be estimated. MC Broyden's method required the largest number of MC samples with our small data and did not give standard errors for the parameters directly. We studied the performance of three different convergence criteria for the MC AI REML algorithm. Our results indicate the importance of defining a suitable convergence criterion and critical value in order to obtain an efficient Newton-type method utilizing a MC algorithm. Overall, use of a MC algorithm with Newton-type methods proved feasible and the results encourage testing of these methods with different kinds of large-scale problem settings.</p>
</abstract>
<funding-group><funding-statement>The study was funded by Rothamsted Research (URL: <ext-link ext-link-type="uri" xlink:href="http://www.rothamsted.ac.uk" xlink:type="simple">www.rothamsted.ac.uk</ext-link>) and MTT Agrifood Research Finland (URL: <ext-link ext-link-type="uri" xlink:href="http://www.mtt.fi" xlink:type="simple">www.mtt.fi</ext-link>). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="7" /></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>Estimation of variance components (VC) by restricted maximum likelihood (REML) <xref ref-type="bibr" rid="pone.0080821-Patterson1">[1]</xref> via a Monte Carlo (MC) expectation maximization (EM) algorithm has proven a computationally attractive choice for large data sets and complex linear mixed effects models <xref ref-type="bibr" rid="pone.0080821-GarcaCorts1">[2]</xref>–<xref ref-type="bibr" rid="pone.0080821-Matilainen1">[4]</xref>. In such cases, it is often impossible to calculate the exact inverse of the coefficient matrix using direct methods, but it can be estimated by MC sampling methods instead. Although the idea of MC EM REML is simple, its convergence is slow, like typical for the EM algorithm. There are different ways to enhance the convergence. One possibility is to use observed information obtained by Louis' method <xref ref-type="bibr" rid="pone.0080821-Louis1">[5]</xref>, which also gives standard errors for the estimates. The MC technique can be adapted to Louis's method as well <xref ref-type="bibr" rid="pone.0080821-Wei1">[6]</xref>. Other possibilities include Aitken's acceleration and quasi-Newton EM acceleration, as used in <xref ref-type="bibr" rid="pone.0080821-Mntysaari1">[7]</xref> and discussed, e.g., in <xref ref-type="bibr" rid="pone.0080821-Jamshidian1">[8]</xref>. However, both Louis' method and the acceleration methods require complicated calculations which may be difficult with the large-scale problems often occurring in animal breeding evaluations.</p>
<p>Newton-type methods are based on second derivatives and reach fast convergence in the neighbourhood of the maximum. Second derivatives with respect to all the parameters yield the information matrix, which can be used to calculate standard errors for the parameters. The Newton-Raphson (NR) method is based on the observed information matrix while Fisher's scoring uses the expected information matrix. Other Newton-type methods include average information (AI) REML, which utilizes the average of the observed and expected information matrices <xref ref-type="bibr" rid="pone.0080821-Gilmour1">[9]</xref>. This is currently the most common VC estimation method used in animal breeding. Quasi-Newton methods <xref ref-type="bibr" rid="pone.0080821-Nocedal1">[10]</xref>, which rely on approximation of second derivatives based on the direction of the most recent step, have also been suggested and used, e.g <xref ref-type="bibr" rid="pone.0080821-Groeneveld1">[11]</xref>. These methods usually result in faster convergence compared to linear methods but slower convergence compared to Newton-type methods because the information matrix is replaced by an approximation.</p>
<p>MC techniques are useful for analyses involving complex likelihoods. Thus, the MC method has been used in the NR algorithm for generalized linear mixed effects models, e.g., by Kuk and Cheng <xref ref-type="bibr" rid="pone.0080821-Kuk1">[12]</xref>, and for incomplete data, e.g., by Gauderman and Navidi <xref ref-type="bibr" rid="pone.0080821-Gauderman1">[13]</xref>. More complicated models related to these examples require simulations from the conditional distribution of the missing data given the observed data with methods like Gibbs sampling. However, the problem in animal breeding is not necessarily the complexity of the model, but rather the need to analyze large-scale data sets to obtain sufficiently accurate genetic parameter estimates. In such cases, the simple sampling method presented in García-Corts et al. <xref ref-type="bibr" rid="pone.0080821-GarcaCorts2">[14]</xref> has shown to be practical for VC estimation in linear mixed effects models by MC EM REML <xref ref-type="bibr" rid="pone.0080821-Matilainen1">[4]</xref>. Its use is also possible in Newton-type methods.</p>
<p>The aim of this study is to compare MC algorithms in different Newton-type methods for VC estimation of linear mixed effects models. We first introduce the AI REML and Broyden's method with MC. These methods are then compared with a sampling-based NR method (MC NR REML), where a simple approximation of second derivatives is possible from independent and identically distributed samples. Finally, we evaluate the performance of these Newton-type methods using the MC algorithm in an analysis of simulated example data.</p>
</sec><sec id="s2" sec-type="materials|methods">
<title>Materials and Methods</title>
<sec id="s2a">
<title>Model</title>
<p>Consider a bivariate linear mixed effects model<disp-formula id="pone.0080821.e001"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0080821.e001" xlink:type="simple" orientation="portrait" /><label>(1)</label></disp-formula>where <bold>y</bold> is a vector of observations, <bold>b</bold> is a vector of fixed effects, <bold>u</bold> is a vector of random effects, <bold>e</bold> is a vector of random error terms or residuals, and <bold>X</bold> and <bold>Z</bold> are design matrices for fixed and random effects, respectively. Assume that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e002" xlink:type="simple" /></inline-formula> has a covariance structure <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e003" xlink:type="simple" /></inline-formula>, where <bold>A</bold> is a numerator relationship matrix and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e004" xlink:type="simple" /></inline-formula> is a 2×2 covariance matrix. Similarly, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e005" xlink:type="simple" /></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e006" xlink:type="simple" /></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e007" xlink:type="simple" /></inline-formula> is a 2×2 covariance matrix. Thus, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e008" xlink:type="simple" /></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e009" xlink:type="simple" /></inline-formula>. We assume that either both or no traits are observed.</p>
</sec><sec id="s2b">
<title>Methods</title>
<p>Let the parameter vector of covariances be <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e010" xlink:type="simple" /></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e011" xlink:type="simple" /></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e012" xlink:type="simple" /></inline-formula> and <italic>vech</italic> is an operator changing unique elements of the matrix argument into vector form. In our case, <italic>θ</italic> is a 6×1 vector which contains three unique elements from both the random effect and residual covariance matrices. Newton-type methods rely on first and second derivatives of the REML likelihood function <italic>L</italic>(<italic>θ</italic>) with respect to <italic>θ</italic>. For example, the NR algorithm uses the observed information matrix<disp-formula id="pone.0080821.e013"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0080821.e013" xlink:type="simple" orientation="portrait" /><label>(2)</label></disp-formula>and the gradient vector<disp-formula id="pone.0080821.e014"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0080821.e014" xlink:type="simple" orientation="portrait" /><label>(3)</label></disp-formula>in calculating new estimates of parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e015" xlink:type="simple" /></inline-formula> at iteration round <italic>k</italic>:<disp-formula id="pone.0080821.e016"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0080821.e016" xlink:type="simple" orientation="portrait" /></disp-formula>where information matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e017" xlink:type="simple" /></inline-formula> and gradient vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e018" xlink:type="simple" /></inline-formula> are computed at current VC estimate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e019" xlink:type="simple" /></inline-formula>.</p>
<p>First derivatives of the REML log-likelihood <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e020" xlink:type="simple" /></inline-formula> with respect to elements in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e021" xlink:type="simple" /></inline-formula> or <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e022" xlink:type="simple" /></inline-formula> can be considered simultaneously <xref ref-type="bibr" rid="pone.0080821-Gauderman1">[13]</xref>. Thus, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e023" xlink:type="simple" /></inline-formula> can be written as a 2×2 matrix which has diagonal elements <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e024" xlink:type="simple" /></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e025" xlink:type="simple" /></inline-formula> and off-diagonal elements <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e026" xlink:type="simple" /></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e027" xlink:type="simple" /></inline-formula>:<disp-formula id="pone.0080821.e028"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0080821.e028" xlink:type="simple" orientation="portrait" /></disp-formula>where <italic>q</italic> is the number of levels in random effect <bold>u</bold>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e029" xlink:type="simple" /></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e030" xlink:type="simple" /></inline-formula> are 2×2 matrices with elements <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e031" xlink:type="simple" /></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e032" xlink:type="simple" /></inline-formula>, respectively. Here <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e033" xlink:type="simple" /></inline-formula> is a subvector of <bold>u</bold> corresponding to the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e034" xlink:type="simple" /></inline-formula> trait in the model, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e035" xlink:type="simple" /></inline-formula> is the part of the inverse of the coefficient matrix of the mixed model equations (MME) corresponding to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e036" xlink:type="simple" /></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e037" xlink:type="simple" /></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e038" xlink:type="simple" /></inline-formula>. Similarly,<disp-formula id="pone.0080821.e039"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0080821.e039" xlink:type="simple" orientation="portrait" /></disp-formula>where <italic>n</italic> is the number of observations, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e040" xlink:type="simple" /></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e041" xlink:type="simple" /></inline-formula> are 2×2 matrices with elements <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e042" xlink:type="simple" /></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e043" xlink:type="simple" /></inline-formula>. Now <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e044" xlink:type="simple" /></inline-formula> is a submatrix of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e045" xlink:type="simple" /></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e046" xlink:type="simple" /></inline-formula> is a subvector of <bold>e</bold> corresponding to the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e047" xlink:type="simple" /></inline-formula> trait, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e048" xlink:type="simple" /></inline-formula> is the part of the inverse of the coefficient matrix of MME corresponding to traits <italic>i</italic> and <italic>j</italic>.</p>
<p>Matrices <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e049" xlink:type="simple" /></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e050" xlink:type="simple" /></inline-formula> are difficult to compute for large data sets and complex models because they require elements of <bold>C</bold>, the inverse of the coefficient matrix of MME. These matrices can be approximated by simulating <italic>s</italic> MC samples of data, i.e., <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e051" xlink:type="simple" /></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e052" xlink:type="simple" /></inline-formula> <xref ref-type="bibr" rid="pone.0080821-GarcaCorts2">[14]</xref> where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e053" xlink:type="simple" /></inline-formula> is a vector of MC simulated observations at MC sample <italic>h</italic>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e054" xlink:type="simple" /></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e055" xlink:type="simple" /></inline-formula> are simulated from their assumed normal density models using current values of the variance parameters. When the full model (1) is fitted, i.e., when MME are solved using the simulated data to obtain estimates <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e056" xlink:type="simple" /></inline-formula>, element (i,j) in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e057" xlink:type="simple" /></inline-formula> can be approximated by method 1 or 2 in García-Corts et al. <xref ref-type="bibr" rid="pone.0080821-GarcaCorts3">[16]</xref>:<disp-formula id="pone.0080821.e058"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0080821.e058" xlink:type="simple" orientation="portrait" /><label>(4)</label></disp-formula>or<disp-formula id="pone.0080821.e059"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0080821.e059" xlink:type="simple" orientation="portrait" /><label>(5)</label></disp-formula>respectively. These formulas are also convenient for multivariate models, as shown in Matilainen et al. <xref ref-type="bibr" rid="pone.0080821-Matilainen1">[4]</xref>. An increase in MC sample size <italic>s</italic> will give more accurate estimates of the <bold>C</bold> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e060" xlink:type="simple" /></inline-formula> matrices and, subsequently, more accurate estimates of the gradient.</p>
<p>Elements in the observed information matrix (2) require more complex calculations than those needed to calculate the gradient vector (3). Approximations are used here to avoid calculations of exact second derivatives. In the following section we present three methods applying the MC sampling scheme. The first method, which is named MC NR REML, is based on calculation of the observed information matrix by sampling. The second method, named MC AI REML, uses MC sampling in the AI REML algorithm. Finally, the third MC sampling method, named MC BM REML, is based on Broyden's method.</p>
<p>MC NR REML. By definition, the expected information matrix at convergence is<disp-formula id="pone.0080821.e061"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0080821.e061" xlink:type="simple" orientation="portrait" /></disp-formula>Use of the MC algorithm with independent and identically distributed samples enables approximation of the information matrix by the variances of the gradients over the samples within each NR REML round. Note, however, that (4) needs to be used to compute the sampling variance of the gradients, because (5) only gives the variances of prediction error variances. Now, the information matrix is approximated by<disp-formula id="pone.0080821.e062"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0080821.e062" xlink:type="simple" orientation="portrait" /></disp-formula>where<disp-formula id="pone.0080821.e063"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0080821.e063" xlink:type="simple" orientation="portrait" /></disp-formula>is a gradient vector calculated based on sample <italic>h</italic>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e064" xlink:type="simple" /></inline-formula>. For a <italic>s</italic>×6 matrix <bold>J</bold>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e065" xlink:type="simple" /></inline-formula> returns a 6×6 matrix, where the diagonal has variance within each column in <bold>J</bold>, and the off-diagonals contain the covariances between each two-column combinations in <bold>J</bold>.</p>
<p>MC AI REML. Johnson and Thompson <xref ref-type="bibr" rid="pone.0080821-Johnson1">[17]</xref> and Gilmour et al. <xref ref-type="bibr" rid="pone.0080821-Gilmour1">[9]</xref> presented AI REML noting that computation of the average of the observed and expected information matrices is easier than of either of the components:<disp-formula id="pone.0080821.e066"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0080821.e066" xlink:type="simple" orientation="portrait" /></disp-formula>where<disp-formula id="pone.0080821.e067"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0080821.e067" xlink:type="simple" orientation="portrait" /></disp-formula>Define <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e068" xlink:type="simple" /></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e069" xlink:type="simple" /></inline-formula> is<disp-formula id="pone.0080821.e070"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0080821.e070" xlink:type="simple" orientation="portrait" /></disp-formula>Then<disp-formula id="pone.0080821.e071"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0080821.e071" xlink:type="simple" orientation="portrait" /></disp-formula>where<disp-formula id="pone.0080821.e072"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0080821.e072" xlink:type="simple" orientation="portrait" /></disp-formula>Hence, in MC AI REML, MC sampling is needed only for estimation of first derivatives in <bold>F</bold>, while the average information can be calculated based on current VC estimates. However, the algorithm requires additional computations to form <bold>F</bold>, which necessitates solving <bold>T</bold> from the MME with data replaced by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e073" xlink:type="simple" /></inline-formula>. Thus, MME needs to be solved for each VC parameter.</p>
<p>MC BM REML. Broyden's method is a quasi-Newton method for numerical solution of non-linear equations <xref ref-type="bibr" rid="pone.0080821-Broyden1">[18]</xref>. It is a generalization of the secant method to multiple dimensions. Broyden's method updates the inverse of the information matrix (instead of the information matrix itself) within each round:<disp-formula id="pone.0080821.e074"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0080821.e074" xlink:type="simple" orientation="portrait" /></disp-formula>where<disp-formula id="pone.0080821.e075"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0080821.e075" xlink:type="simple" orientation="portrait" /></disp-formula><disp-formula id="pone.0080821.e076"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0080821.e076" xlink:type="simple" orientation="portrait" /></disp-formula>Instead of using true gradients <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e077" xlink:type="simple" /></inline-formula> for both the update of the inverse information matrix and the update of new estimates, we used the round-to-round changes in the EM estimates <xref ref-type="bibr" rid="pone.0080821-Jamshidian2">[19]</xref>:<disp-formula id="pone.0080821.e078"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0080821.e078" xlink:type="simple" orientation="portrait" /></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e079" xlink:type="simple" /></inline-formula> is a vector of EM REML solutions computed from the estimates from round <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e080" xlink:type="simple" /></inline-formula>. Apart from scaling, they are relative to the original gradients. In the beginning, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e081" xlink:type="simple" /></inline-formula> is identity matrix <bold>I</bold>. The first update of the inverse of the information matrix is made at round <italic>k</italic> = 2 based on estimates from the first round <italic>k</italic> = 1 and initial values at <italic>k</italic> = 0.</p>
<p>This method leads to superlinear convergence although sequence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e082" xlink:type="simple" /></inline-formula> does not converge to the observed information matrix at the maximum <xref ref-type="bibr" rid="pone.0080821-Dennis1">[20]</xref>. Like with MC AI REML, MC sampling is not used to estimate the information matrix directly. Instead, the sampling variation comes in through the gradients which are used to update the inverse of the information matrix within each round.</p>
</sec><sec id="s2c">
<title>Analysis of test data</title>
<p>For this study we simulated a small data set which mimics a typical set-up in dairy cattle breeding. The two study traits resembled 305-day milk and fat yield records in 20 herds. The base generation comprised 146 unrelated sires, each of which had 1 to 10 daughters with unknown and unrelated dams. Each daughter had one observation of the two traits, and the data contained 569 observations for both traits. The pedigree comprised a total of 715 animals. Fixed herd effects and random genetic animal effects were included in the study model. <xref ref-type="table" rid="pone-0080821-t001">Table 1</xref> shows genetic and residual VC in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e083" xlink:type="simple" /></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e084" xlink:type="simple" /></inline-formula> used to simulate 305-day milk and fat records. The simulation of observation was based on the assumed linear mixed effects model and VCs.</p>
<table-wrap id="pone-0080821-t001" position="float" orientation="portrait"><object-id pub-id-type="doi">10.1371/journal.pone.0080821.t001</object-id><label>Table 1</label><caption>
<title>Variance components used for the simulation, initial values used for the analyses and estimates by analytical EM REML.</title>
</caption><alternatives><graphic id="pone-0080821-t001-1" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0080821.t001" xlink:type="simple" orientation="portrait" />
<table><colgroup span="1"><col align="left" span="1" /><col align="center" span="1" /><col align="center" span="1" /><col align="center" span="1" /><col align="center" span="1" /><col align="center" span="1" /><col align="center" span="1" /></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1" />
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e085" xlink:type="simple" /></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e086" xlink:type="simple" /></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e087" xlink:type="simple" /></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e088" xlink:type="simple" /></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e089" xlink:type="simple" /></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e090" xlink:type="simple" /></inline-formula></td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Simulation value</td>
<td align="left" rowspan="1" colspan="1">500.0</td>
<td align="left" rowspan="1" colspan="1">14.00</td>
<td align="left" rowspan="1" colspan="1">0.800</td>
<td align="left" rowspan="1" colspan="1">750.0</td>
<td align="left" rowspan="1" colspan="1">29.00</td>
<td align="left" rowspan="1" colspan="1">1.400</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Initial value</td>
<td align="left" rowspan="1" colspan="1">350.3</td>
<td align="left" rowspan="1" colspan="1">12.18</td>
<td align="left" rowspan="1" colspan="1">0.599</td>
<td align="left" rowspan="1" colspan="1">615.8</td>
<td align="left" rowspan="1" colspan="1">21.34</td>
<td align="left" rowspan="1" colspan="1">1.061</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">EM REML</td>
<td align="left" rowspan="1" colspan="1">511.9</td>
<td align="left" rowspan="1" colspan="1">18.11</td>
<td align="left" rowspan="1" colspan="1">0.747</td>
<td align="left" rowspan="1" colspan="1">842.6</td>
<td align="left" rowspan="1" colspan="1">29.10</td>
<td align="left" rowspan="1" colspan="1">1.590</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">NR REML</td>
<td align="left" rowspan="1" colspan="1">512.1</td>
<td align="left" rowspan="1" colspan="1">18.20</td>
<td align="left" rowspan="1" colspan="1">0.730</td>
<td align="left" rowspan="1" colspan="1">842.3</td>
<td align="left" rowspan="1" colspan="1">29.02</td>
<td align="left" rowspan="1" colspan="1">1.607</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">AI REML</td>
<td align="left" rowspan="1" colspan="1">512.1</td>
<td align="left" rowspan="1" colspan="1">18.20</td>
<td align="left" rowspan="1" colspan="1">0.730</td>
<td align="left" rowspan="1" colspan="1">842.3</td>
<td align="left" rowspan="1" colspan="1">29.02</td>
<td align="left" rowspan="1" colspan="1">1.607</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">BM REML</td>
<td align="left" rowspan="1" colspan="1">512.6</td>
<td align="left" rowspan="1" colspan="1">18.08</td>
<td align="left" rowspan="1" colspan="1">0.751</td>
<td align="left" rowspan="1" colspan="1">841.9</td>
<td align="left" rowspan="1" colspan="1">29.13</td>
<td align="left" rowspan="1" colspan="1">1.586</td>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt101"><p><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e091" xlink:type="simple" /></inline-formula>) and three unique residual (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e092" xlink:type="simple" /></inline-formula>) (co)variance components. All values are presented in thousands.<sup /> The model includes three unique genetic (</p></fn></table-wrap-foot></table-wrap>
<p>All the algorithms used for analyzing the data were implemented in R software <xref ref-type="bibr" rid="pone.0080821-R1">[21]</xref>. First we applied analytical EM REML, NR REML, AI REML and BM REML. For these analytical analyses we used convergence criteria based on relative squared changes in consecutive estimates, with 10<sup>−10</sup> as the critical value. Each MC REML algorithm was tested with 20, 100 and 1000 MC samples per REML iteration round. The MC EM REML algorithm with 20 MC samples per REML round was used as a reference <xref ref-type="bibr" rid="pone.0080821-Matilainen1">[4]</xref>. Estimates from round 2 of MC EM REML analysis were set as initial values for the Newton-type analyses (<xref ref-type="table" rid="pone-0080821-t001">Table 1</xref>). For cases where Newton-type algorithms yielded estimates outside the parameter space, crash recovery was implemented by weighting the Newton-type and EM REML estimates with a weighting factor sequentially from 0.1 to 1.0 by 0.1 until the estimated VC matrices were positive-definite <xref ref-type="bibr" rid="pone.0080821-Jensen1">[15]</xref>.</p>
<p>Convergence of an MC algorithm is difficult to identify, and so we examined the convergence performance of MC REML algorithms by continuing an additional 10 REML rounds more than required by corresponding analytical analyses. The obtained mean and relative standard deviations of the parameter estimates over the additional REML rounds are shown in <xref ref-type="table" rid="pone-0080821-t002">Table 2</xref>. Three convergence criteria presented in the literature were then calculated for the MC AI REML algorithm. The first is a commonly used criterion, presented by Booth and Hobert <xref ref-type="bibr" rid="pone.0080821-Booth1">[22]</xref>, which is based on a change in consecutive parameter estimates relative to their standard errors. A value of 0.005 can be used as the critical value. The second criterion, by Kuk and Cheng <xref ref-type="bibr" rid="pone.0080821-Kuk1">[12]</xref>, relies on the gradient vector and its variance-covariance matrix. Their stopping criterion is 90-percent quantile of a chi-square distribution with the number of parameters as degrees of freedom. This criterion attempts to stop the iteration as soon as possible. Finally, from MC AI REML round 5 onwards, convergence was also checked by a method similar to the one in Matilainen et al. <xref ref-type="bibr" rid="pone.0080821-Matilainen1">[4]</xref>, where the approach was to predict the parameter estimates of the next round using linear regression on previous iteration rounds. Here we took the same approach but applied the prediction method to the gradients instead of the estimates. Analyses were continued until the critical value of 10<sup>−10</sup> as a norm for predicted round-to-round change in the gradient was reached.</p>
<table-wrap id="pone-0080821-t002" position="float" orientation="portrait"><object-id pub-id-type="doi">10.1371/journal.pone.0080821.t002</object-id><label>Table 2</label><caption>
<title>Means (relative standard deviation) of estimates over the last 10 rounds by MC REML.</title>
</caption><alternatives><graphic id="pone-0080821-t002-2" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0080821.t002" xlink:type="simple" orientation="portrait" />
<table><colgroup span="1"><col align="left" span="1" /><col align="center" span="1" /><col align="center" span="1" /><col align="center" span="1" /><col align="center" span="1" /><col align="center" span="1" /><col align="center" span="1" /></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">Method</td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e093" xlink:type="simple" /></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e094" xlink:type="simple" /></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e095" xlink:type="simple" /></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e096" xlink:type="simple" /></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e097" xlink:type="simple" /></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e098" xlink:type="simple" /></inline-formula></td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">EM 20</td>
<td align="left" rowspan="1" colspan="1">519.3 (0.5%)</td>
<td align="left" rowspan="1" colspan="1">18.30 (0.5%)</td>
<td align="left" rowspan="1" colspan="1">0.752 (0.4%)</td>
<td align="left" rowspan="1" colspan="1">843.3 (1.1%)</td>
<td align="left" rowspan="1" colspan="1">28.98 (1.0%)</td>
<td align="left" rowspan="1" colspan="1">1.578 (1.0%)</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">NR 20</td>
<td align="left" rowspan="1" colspan="1">446.8 (60.8%)</td>
<td align="left" rowspan="1" colspan="1">15.54 (71.8%)</td>
<td align="left" rowspan="1" colspan="1">0.653 (67.5%)</td>
<td align="left" rowspan="1" colspan="1">877.3 (32.4%)</td>
<td align="left" rowspan="1" colspan="1">30.70 (35.4%)</td>
<td align="left" rowspan="1" colspan="1">1.655 (25.3%)</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">NR 100</td>
<td align="left" rowspan="1" colspan="1">509.8 (5.4%)</td>
<td align="left" rowspan="1" colspan="1">17.91 (6.4%)</td>
<td align="left" rowspan="1" colspan="1">0.712 (7.7%)</td>
<td align="left" rowspan="1" colspan="1">842.3 (2.6%)</td>
<td align="left" rowspan="1" colspan="1">29.20 (3.4%)</td>
<td align="left" rowspan="1" colspan="1">1.620 (3.3%)</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">NR 1000</td>
<td align="left" rowspan="1" colspan="1">510.9 (1.6%)</td>
<td align="left" rowspan="1" colspan="1">18.18 (2.1%)</td>
<td align="left" rowspan="1" colspan="1">0.730 (2.5%)</td>
<td align="left" rowspan="1" colspan="1">843.3 (0.8%)</td>
<td align="left" rowspan="1" colspan="1">29.04 (1.0%)</td>
<td align="left" rowspan="1" colspan="1">1.607 (0.9%)</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">AI 20</td>
<td align="left" rowspan="1" colspan="1">495.5 (7.2%)</td>
<td align="left" rowspan="1" colspan="1">17.44 (8.1%)</td>
<td align="left" rowspan="1" colspan="1">0.689 (8.4%)</td>
<td align="left" rowspan="1" colspan="1">855.3 (3.4%)</td>
<td align="left" rowspan="1" colspan="1">29.57 (4.5%)</td>
<td align="left" rowspan="1" colspan="1">1.632 (4.0%)</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">AI 100</td>
<td align="left" rowspan="1" colspan="1">513.4 (4.2%)</td>
<td align="left" rowspan="1" colspan="1">18.20 (4.7%)</td>
<td align="left" rowspan="1" colspan="1">0.729 (5.2%)</td>
<td align="left" rowspan="1" colspan="1">839.9 (2.6%)</td>
<td align="left" rowspan="1" colspan="1">28.93 (2.8%)</td>
<td align="left" rowspan="1" colspan="1">1.602 (2.4%)</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">AI 1000</td>
<td align="left" rowspan="1" colspan="1">513.8 (1.6%)</td>
<td align="left" rowspan="1" colspan="1">18.28 (1.9%)</td>
<td align="left" rowspan="1" colspan="1">0.734 (1.9%)</td>
<td align="left" rowspan="1" colspan="1">840.3 (0.9%)</td>
<td align="left" rowspan="1" colspan="1">28.92 (1.1%)</td>
<td align="left" rowspan="1" colspan="1">1.603 (0.8%)</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">BM 1000</td>
<td align="left" rowspan="1" colspan="1">502.1 (3.2%)</td>
<td align="left" rowspan="1" colspan="1">17.73 (3.5%)</td>
<td align="left" rowspan="1" colspan="1">0.758 (1.1%)</td>
<td align="left" rowspan="1" colspan="1">852.7 (1.9%)</td>
<td align="left" rowspan="1" colspan="1">29.48 (1.9%)</td>
<td align="left" rowspan="1" colspan="1">1.581 (0.5%)</td>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt102"><p><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e099" xlink:type="simple" /></inline-formula>) and three unique residual (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e100" xlink:type="simple" /></inline-formula>) (co)variance components. Values were calculated over REML rounds 402 to 411 for MC EM REML, 6 to 15 for MC NR and MC AI REML, and 12 to 21 for MC BM REML with 20, 100 or 1000 MC samples. Mean values are presented in thousands.<sup /> The model includes three unique genetic (</p></fn></table-wrap-foot></table-wrap></sec></sec><sec id="s3">
<title>Results</title>
<p>Analytical EM REML converged in 401 rounds, analytical NR REML and AI REML in 5 rounds, and analytical BM REML in 11 rounds. Estimates by analytical algorithms differed by less than 3% across algorithms, as seen in <xref ref-type="table" rid="pone-0080821-t001">Table 1</xref>. The mean and relative standard deviation for the MC REML estimates obtained from the additional 10 REML rounds after reaching the convergence point determined by corresponding analytical algorithms are given in <xref ref-type="table" rid="pone-0080821-t002">Table 2</xref>. Due to convergence problems in MC BM REML, only results with 1000 MC samples per REML rounds are reported here(<xref ref-type="table" rid="pone-0080821-t002">Table 2</xref>). Almost all VC estimates were in good agreement with the analytical estimates, their means deviating less than 2.5% from the analytical ones. The exceptions were estimates by MC NR REML with 20 samples and those for genetic effect by MC AI REML with 20 samples. The variability of the estimates can be seen in the relative standard deviations over the last 10 REML rounds. Round-to-round variation in the MC EM REML estimates after assumed convergence was only 0.5% for genetic VC, while MC NR REML and MC AI REML with 100 samples per REML round would still have relative standard errors of 5%–8% and 4%–5%, respectively, in the corresponding estimates.</p>
<p>MC REML round-to-round convergence in the genetic covariance component <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e101" xlink:type="simple" /></inline-formula> is illustrated in <xref ref-type="fig" rid="pone-0080821-g001">Fig. 1</xref>. The straight lines in the figures represent the estimated genetic covariance (solid line) and estimated standard error (dashed lines) by analytical AI REML. <xref ref-type="fig" rid="pone-0080821-g002">Fig. 2</xref> describes the relative absolute difference between estimates obtained by MC AI REML with different numbers of MC samples and the true estimate by analytical AI REML.</p>
<fig id="pone-0080821-g001" position="float" orientation="portrait"><object-id pub-id-type="doi">10.1371/journal.pone.0080821.g001</object-id><label>Figure 1</label><caption>
<title>Estimates of the genetic covariance component by Newton-type methods.</title>
<p>Analyses by MC NR REML (Figure A), MC AI REML (Figure B) and MC BM REML (Figure C) with 20, 100 and 1000 MC samples (green, blue and red line, respectively). MC EM REML with 20 MC samples is plotted as a reference (grey line). The straight lines in the figures are the estimated genetic covariance (solid line) and plus/minus one standard deviation (dashed lines) based on standard errors by analytical AI REML.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0080821.g001" position="float" xlink:type="simple" orientation="portrait" /></fig><fig id="pone-0080821-g002" position="float" orientation="portrait"><object-id pub-id-type="doi">10.1371/journal.pone.0080821.g002</object-id><label>Figure 2</label><caption>
<title>Relative difference between MC AI REML estimates and the true estimate obtained by analytical AI REML.</title>
<p>The relative difference (%) is plotted for MC AI REML estimates with 20, 100 and 1000 MC samples (green, blue and red line, respectively) along the iteration. MC EM REML with 20 MC samples is plotted as a reference (grey line).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0080821.g002" position="float" xlink:type="simple" orientation="portrait" /></fig>
<p>The standard error of the genetic covariance estimates were 7996 and 8274 by the analytical NR REML and AI REML algorithms, respectively. Standard errors were not calculated by analytical EM REML and BM REML. When calculated by MC NR REML, standard errors for the genetic covariances at REML round 10 was 11360, 8056 and 8495 with 20, 100 and 1000 MC samples per round, respectively. Corresponding, standard errors by MC AI REML at REML round 10 were 8857, 8185 and 8294 with 20, 100 and 1000 MC samples per REML round. However, it should be noted that these actual numbers of standard errors may vary from round to round due to sampling.</p>
<p>Of the three different convergence criteria studied for the MC AI REML algorithm, the convergence criterion presented by Booth and Hobert <xref ref-type="bibr" rid="pone.0080821-Booth1">[22]</xref> gave average values of 0.35, 0.15 and 0.05 with 20, 100 and 1000 MC samples per MC AI REML round, respectively. This indicates the need for a huge increase in MC sample size before the critical value of 0.005 proposed by Booth and Hobert <xref ref-type="bibr" rid="pone.0080821-Booth1">[22]</xref> can be reached. Kuk and Cheng <xref ref-type="bibr" rid="pone.0080821-Kuk1">[12]</xref>, in turn, suggest stopping the iteration at MC AI REML rounds 2, 1 and 1 with 20, 100 and 1000 MC samples per round, respectively. Their criterion implies relatively small gradients after 1 or 2 steps which is probably due to large standard errors of the estimates. According to the convergence criterion in Matilainen et al. <xref ref-type="bibr" rid="pone.0080821-Matilainen1">[4]</xref> using a critical value of 10<sup>−10</sup>, iteration would stop at MC AI REML rounds 101, 70 and 44 with 20, 100 and 1000 MC samples per round, respectively. Because this criterion may be too strict for practical purposes in MC REML analyses, we also checked stopping at points when the criterion gave values less than 10<sup>−8</sup>. This would mean that analyses would be stopped at MC AI REML rounds 28, 27 and 10 with 20, 100 and 1000 MC samples, respectively.</p>
</sec><sec id="s4">
<title>Discussion</title>
<p>Whereas the MC NR REML method is easy to implement, it may require a large number of MC samples to accurately approximate the variances of first derivatives over samples. MC AI REML, in contrast, works better even with small MC sample sizes, because the AI matrix has no extra sampling noise as it depends only on variance parameters estimated in the previous round. MC AI REML rounds are computationally more demanding than MC NR REML, however, because the MME system needs to be solved at each MC AI REML round as many times as there are VC parameters to be estimated.</p>
<p>MC BM REML is computationally the least expensive of the considered methods when the number of REML rounds and the number of MC samples are kept the same. To circumvent evaluation of the information matrix, BM REML corrects the approximation of the inverse of information matrix from round to round based on the gradients. While the analytical BM REML worked reasonably well, the small data set in our study required a large MC sample size for the method to work, which indicates its sensitivity to changes in gradients from round to round. Furthermore, MC BM REML is efficient even with a fairly poor approximation to the information matrix, but extra computations are needed for standard errors after convergence has been reached.</p>
<p>The performance of MC NR and MC AI REML was quite similar to analytical NR and AI REML. The only clear difference was that, with small MC sample sizes, estimates by MC NR REML varied more than those by MC AI REML. With 20 MC samples, the relative standard deviations from the last 10 REML rounds by both methods were unacceptably high, although MC AI REML was better. With 100 MC samples per REML round, the standard deviations were acceptable, and estimates by MC NR REML showed approximately as much variation as the estimates of MC AI REML. Thus, the information matrix appears to be quite accurately estimated in this case. With 1000 MC samples, variation in MC NR REML and MC AI REML estimates was almost equal. Genetic covariance estimates by both methods differed on average from the true value by 5% and 2% with 100 and 1000 MC samples, respectively. Interestingly, such variation diminished when MC BM REML was applied (<xref ref-type="fig" rid="pone-0080821-g001">Fig. 1c</xref>). Why this did not happen with MC NR or MC AI REML analysis may be because the diagonals in the approximation of the inverse of the information matrix were close to unity throughout the analysis, leading to more like MC EM REML parameter estimates.</p>
<p>For analytical REML analysis, Newton-type REML algorithms provide much faster convergence than EM REML, leading to shorter overall solving times with small data sets. The use of MC in the algorithms speeds up convergence of Newton-type methods, but sampling variation in the estimates increases compared to MC EM REML analysis. This is due to multiplication of the gradients by the inverse of the information matrix, as seen in the increase of MC noise. If each round of iteration in Newton-type methods requires many more samples than MC EM REML, overall solving time will reduced only in case the Newton method can enhance the convergence dramatically. The solving times were not recorded in this study because they would only apply to the model and implementation used. With respect to the total number of times to solve MME along the analysis, results showed that MC EM REML with 20 MC samples and 401 EM REML rounds corresponded to MC NR REML with 100 MC samples and 80 NR REML rounds or MC AI REML with 100 MC samples and 75 AI REML rounds. Thus, the number of times required to solve MME within a REML round is <italic>s</italic>+1 for MC NR REML but <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e102" xlink:type="simple" /></inline-formula> for MC AI REML, where <italic>s</italic> is the number of MC samples and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0080821.e103" xlink:type="simple" /></inline-formula> is the number of VC parameters. Hence, in our example with six parameters, MC NR and MC AI REML clearly outperformed MC EM REML, especially if we consider that the analytically implemented NR REML and AI REML needed 5 REML rounds to reach convergence but EM REML needed 401 rounds.</p>
<p>Obtaining a fast algorithm for REML estimation requires development of a practical convergence criterion for Newton-type methods. Although convergence is the same regardless of MC sample size, MC variation affects the values of the convergence criteria. Further study is therefore needed to define a suitable critical value for genetic evaluations. Identification of a feasible convergence criterion also requires deciding which values to use as the final solutions: the average of estimates over several REML rounds or simply the estimates at the last REML round.</p>
<p>The performance of MC-based algorithms is the better the larger the data to be analyzed. With a large data set, the averages of the gradients for MC AI REML are more accurate also with a smaller MC sample size, which leads to more accurate moves in the EM steps of MC BM REML. Most probably the amount of MC samples needed for sufficiently accurate gradient variances in MC NR REML will also decrease somewhat. As models grow larger and more complex, the efficiency of different methods becomes more difficult to predict. Further experience is especially needed on the behaviour of MC BM REML in VC estimation of complex models. A shortfall with respect to MC AI REML is that the number of times needed to solve MME increases along with increase in the number of estimated VCs. This fact does not change even with a large data set, and so MC NR and MC EM REML may become more efficient than MC AI REML. For instance, in <xref ref-type="bibr" rid="pone.0080821-Matilainen1">[4]</xref>, MC EM REML was used to estimate 96 VCs in a model describing daily milk yields of dairy cows. Estimation by MC EM REML with 5 MC samples per REML round required 565 rounds. The same analysis by MC AI REML with 20 MC samples per REML round should converge in less than 25 rounds to be computationally superior over MC EM REML, given that the MME solving time is the same for both algorithms.</p>
<p>The estimates of the analyses presented here were weighted by corresponding EM REML estimates whenever they fell outside the parameter space. Yet, this does not guarantee convergence to the true solutions, especially with respect to Broyden's method. To avoid divergence, Broyden <xref ref-type="bibr" rid="pone.0080821-Broyden1">[18]</xref> suggested choosing a scalar multiplier, i.e., a step length that decreases the change in some gradient norm and ensures the ascent of likelihood at each step. Convergence is also guaranteed by the Wolfe conditions <xref ref-type="bibr" rid="pone.0080821-Nocedal1">[10]</xref>, which ensure that steps make a sufficient ascent. However, if the search direction in BM REML approximates the Newton direction well enough, the unit step length will satisfy the Wolfe conditions, as the iterates converge to the solution <xref ref-type="bibr" rid="pone.0080821-Nocedal1">[10]</xref>. Based on our study, this may mean that the required MC sample size may become enormous. One way to increase the robustness of VC estimation algorithms is reparametrization of the VC matrices by Cholesky decomposition <xref ref-type="bibr" rid="pone.0080821-Groeneveld1">[11]</xref>. The performance of this option is worth considering in future studies.</p>
</sec><sec id="s5">
<title>Conclusions</title>
<p>Our results show that the use of MC algorithms in different Newton-type methods for VC estimation is feasible, although there was variation in efficiency between the implementations. An efficient MC method can achieve fast convergence and short computing times for VC estimation in complex linear mixed effects models when sampling techniques are used. However, analysis of our small simulated data implies that the number of MC samples needed for accurate estimation is dependent on the used method. This work encourages testing the performance of the presented methods in solving large-scale problems.</p>
</sec></body>
<back><ref-list>
<title>References</title>
<ref id="pone.0080821-Patterson1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Patterson</surname><given-names>HD</given-names></name>, <name name-style="western"><surname>Thompson</surname><given-names>R</given-names></name> (<year>1971</year>) <article-title>Recovery of inter-block information when block sizes are unequal</article-title>. <source>Biometrika</source> <volume>58</volume>: <fpage>545</fpage>–<lpage>554</lpage>.</mixed-citation>
</ref>
<ref id="pone.0080821-GarcaCorts1"><label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>García-Cortés</surname><given-names>LA</given-names></name>, <name name-style="western"><surname>Sorensen</surname><given-names>D</given-names></name> (<year>2001</year>) <article-title>Alternative implementations of Monte Carlo EM algorithms for likelihood inferences</article-title>. <source>Genet Sel Evol</source> <volume>33</volume>: <fpage>443</fpage>–<lpage>452</lpage>.</mixed-citation>
</ref>
<ref id="pone.0080821-Harville1"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Harville</surname><given-names>DA</given-names></name> (<year>2004</year>) <article-title>Making REML computationally feasible for large data sets: Use of Gibbs sampler</article-title>. <source>J Stat Comput Simul</source> <volume>74</volume>: <fpage>135</fpage>–<lpage>153</lpage>.</mixed-citation>
</ref>
<ref id="pone.0080821-Matilainen1"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Matilainen</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Mäntysaari</surname><given-names>EA</given-names></name>, <name name-style="western"><surname>Lidauer</surname><given-names>MH</given-names></name>, <name name-style="western"><surname>Strandén</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Thompson</surname><given-names>R</given-names></name> (<year>2012</year>) <article-title>Employing a Monte Carlo algorithm in expectation maximization restricted maximum likelihood estimation of the linear mixed effects model</article-title>. <source>J Anim Breed Genet</source> <volume>129</volume>: <fpage>457</fpage>–<lpage>468</lpage>.</mixed-citation>
</ref>
<ref id="pone.0080821-Louis1"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Louis</surname><given-names>TA</given-names></name> (<year>1982</year>) <article-title>Finding the observed information matrix when using the EM algorithm</article-title>. <source>J R Stat Soc Series B Stat Methodol</source> <volume>44</volume>: <fpage>226</fpage>–<lpage>233</lpage>.</mixed-citation>
</ref>
<ref id="pone.0080821-Wei1"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wei</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Tanner</surname><given-names>M</given-names></name> (<year>1990</year>) <article-title>A Monte Carlo implementation of the EM algorithm and the poor man's data augmentation algorithms</article-title>. <source>J Am Stat Assoc</source> <volume>85</volume>: <fpage>699</fpage>–<lpage>704</lpage>.</mixed-citation>
</ref>
<ref id="pone.0080821-Mntysaari1"><label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mäntysaari</surname><given-names>E</given-names></name>, <name name-style="western"><surname>van Vleck</surname><given-names>LD</given-names></name> (<year>1989</year>) <article-title>Restricted maximum likelihood estimates of variance components from multitrait sire models with large number of fixed effects</article-title>. <source>Journal of Animal Breeding and Genetics</source> <volume>106</volume>: <fpage>409</fpage>–<lpage>422</lpage>.</mixed-citation>
</ref>
<ref id="pone.0080821-Jamshidian1"><label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jamshidian</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Jennrich</surname><given-names>RI</given-names></name> (<year>1997</year>) <article-title>Acceleration of the EM algorithm by using quasi-Newton methods</article-title>. <source>J R Stat Soc Series B Stat Methodol</source> <volume>59</volume>: <fpage>569</fpage>–<lpage>587</lpage>.</mixed-citation>
</ref>
<ref id="pone.0080821-Gilmour1"><label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gilmour</surname><given-names>AR</given-names></name>, <name name-style="western"><surname>Thompson</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Cullis</surname><given-names>BR</given-names></name> (<year>1995</year>) <article-title>Average information REML: An efficient algorithm for variance parameter estimation in linear mixed models</article-title>. <source>Biometrics</source> <volume>51</volume>: <fpage>1440</fpage>–<lpage>1450</lpage>.</mixed-citation>
</ref>
<ref id="pone.0080821-Nocedal1"><label>10</label>
<mixed-citation publication-type="book" xlink:type="simple">Nocedal J, Wright SJ (2006) Numerical Optimization. New York: Springer, 2nd edition. 664 p.</mixed-citation>
</ref>
<ref id="pone.0080821-Groeneveld1"><label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Groeneveld</surname><given-names>E</given-names></name> (<year>1994</year>) <article-title>A reparametrization to improve numerical optimization in multivariate REML (co)variance component estimation</article-title>. <source>Genet Sel Evol</source> <volume>26</volume>: <fpage>537</fpage>–<lpage>545</lpage>.</mixed-citation>
</ref>
<ref id="pone.0080821-Kuk1"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kuk</surname><given-names>AYC</given-names></name>, <name name-style="western"><surname>Cheng</surname><given-names>YW</given-names></name> (<year>1997</year>) <article-title>The Monte Carlo Newton-Raphson algorithm</article-title>. <source>J Stat Comput Simul</source> <volume>59</volume>: <fpage>233</fpage>–<lpage>250</lpage>.</mixed-citation>
</ref>
<ref id="pone.0080821-Gauderman1"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gauderman</surname><given-names>WJ</given-names></name>, <name name-style="western"><surname>Navidi</surname><given-names>W</given-names></name> (<year>2001</year>) <article-title>A Monte Carlo Newton-Raphson procedure for maximizing complex likelihoods on pedigree data</article-title>. <source>Comput Stat Data Anal</source> <volume>35</volume>: <fpage>395</fpage>–<lpage>415</lpage>.</mixed-citation>
</ref>
<ref id="pone.0080821-GarcaCorts2"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>García-Cortés</surname><given-names>LA</given-names></name>, <name name-style="western"><surname>Moreno</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Varona</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Altarriba</surname><given-names>J</given-names></name> (<year>1992</year>) <article-title>Variance component estimation by resampling</article-title>. <source>J Anim Breed Genet</source> <volume>109</volume>: <fpage>358</fpage>–<lpage>363</lpage>.</mixed-citation>
</ref>
<ref id="pone.0080821-Jensen1"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jensen</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Mäntysaari</surname><given-names>EA</given-names></name>, <name name-style="western"><surname>Madsen</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Thompson</surname><given-names>R</given-names></name> (<year>1997</year>) <article-title>Residual maximum likelihood estimation of (co)variance components in multivariate mixed linear models using average information</article-title>. <source>J Indian Soc Agric Stat</source> <volume>49</volume>: <fpage>215</fpage>–<lpage>236</lpage>.</mixed-citation>
</ref>
<ref id="pone.0080821-GarcaCorts3"><label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>García-Cortés</surname><given-names>LA</given-names></name>, <name name-style="western"><surname>Moreno</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Varona</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Altarriba</surname><given-names>J</given-names></name> (<year>1995</year>) <article-title>Estimation of prediction-error variances by resampling</article-title>. <source>J Anim Breed Genet</source> <volume>112</volume>: <fpage>176</fpage>–<lpage>182</lpage>.</mixed-citation>
</ref>
<ref id="pone.0080821-Johnson1"><label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Johnson</surname><given-names>DL</given-names></name>, <name name-style="western"><surname>Thompson</surname><given-names>R</given-names></name> (<year>1995</year>) <article-title>Restricted maximum likelihood estimation of variance components for univariate animal models using sparse matrix techniques and average information</article-title>. <source>J Dairy Sci</source> <volume>78</volume>: <fpage>449</fpage>–<lpage>456</lpage>.</mixed-citation>
</ref>
<ref id="pone.0080821-Broyden1"><label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Broyden</surname><given-names>CG</given-names></name> (<year>1965</year>) <article-title>A class of methods for solving nonlinear simultaneous equations</article-title>. <source>Math Comp</source> <volume>19</volume>: <fpage>577</fpage>–<lpage>593</lpage>.</mixed-citation>
</ref>
<ref id="pone.0080821-Jamshidian2"><label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jamshidian</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Jennrich</surname><given-names>RI</given-names></name> (<year>1993</year>) <article-title>Conjugate gradient acceleration of the EM algorithm</article-title>. <source>J Am Stat Assoc</source> <volume>88</volume>: <fpage>221</fpage>–<lpage>228</lpage>.</mixed-citation>
</ref>
<ref id="pone.0080821-Dennis1"><label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dennis</surname><given-names>JE</given-names></name>, <name name-style="western"><surname>Moré</surname><given-names>JJ</given-names></name> (<year>1974</year>) <article-title>A characterization of superlinear convergence and its application to quasi-Newton methods</article-title>. <source>Math Comp</source> <volume>28</volume>: <fpage>549</fpage>–<lpage>560</lpage>.</mixed-citation>
</ref>
<ref id="pone.0080821-R1"><label>21</label>
<mixed-citation publication-type="other" xlink:type="simple">R Core Team (2012) R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria. URL <ext-link ext-link-type="uri" xlink:href="http://www.R-project.org/" xlink:type="simple">http://www.R-project.org/</ext-link>. ISBN 3-900051-07-0.</mixed-citation>
</ref>
<ref id="pone.0080821-Booth1"><label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Booth</surname><given-names>JG</given-names></name>, <name name-style="western"><surname>Hobert</surname><given-names>JP</given-names></name> (<year>1999</year>) <article-title>Maximizing generalized linear mixed model likelihoods with an automated Monte Carlo EM algorithm</article-title>. <source>J R Stat Soc Series B Stat Methodol</source> <volume>61</volume>: <fpage>265</fpage>–<lpage>285</lpage>.</mixed-citation>
</ref>
</ref-list></back>
</article>